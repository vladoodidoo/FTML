\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}
\input{stru/structure.tex}
\hyphenation{Fortran hy-phen-ation} 

\title{{Fondamentaux th√©oriques du Machine Learning (Cours Epita 2022)}} 
\author{\spacedlowsmallcaps{Nicolas Le Hir}} 
\date{}
\begin{document}
\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}}
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} 
\pagestyle{scrheadings}
\maketitle 
\setcounter{tocdepth}{3} 

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{knn}
    \label{fig:knn}
\end{figure}

\tableofcontents

\section*{\large\color{Blue}Presentation}

This document gathers important results and tools that will be used during the FTML course at Epita. It is not intended as an exhaustive course on the topic, as some results are stated without proof. It should rather be seen as a compact ressource of facts, definitions and results that we will use or focus on.

Here is a short list of revelant textbooks. For some of them, the pdf version is freely available online (with sometimes additional content in the physical textbooks).
\begin{itemize}
    \item \cite{Azencott2022}  (in french)

\url{https://cazencott.info/index.php/pages/Introduction-au-Machine-Learning} 
\item \cite{Alpaydin} 
\item \cite{Bach2021} 

\url{https://francisbach.com/i-am-writing-a-book/} 
\item \cite{Shalev-Shwartz2013} 
\item \cite{Cornuejols2003} 
\item \cite{Allaire2012} 
\item \cite{Hastie2009} 
\end{itemize}

These textbooks are also referenced in the file
\url{https://github.com/nlehir/FTML_PTML/blob/master/FTML/references.pdf} 

Many ressources are also available in the documentation of scikit-learn:

\url{https://scikit-learn.org/stable/} 

\section{\large\color{Blue}General mathematical results}

\subsection{\large\color{MidnightBlue}Linear algebra}

\subsubsection{\large\color{Periwinkle}Matricial writing of inner products}
\label{ssec:linalg}

In machine learning, optimization or statistics we often write the inner product of two vectors of $ \mathbb{R}^d$ as a product of matrices. If $x\in \mathbb{R}^d$ writes:
\begin{equation*}
x=\begin{pmatrix}
x_{1}\\
...\\
x_{i}\\
...\\
x_{d}
\end{pmatrix}
\end{equation*}

And (with $T$ denoting the transposition),
\begin{equation*}
y^T=\begin{pmatrix}
y_{1}, \dots y_j, \dots, y_d\\
\end{pmatrix}
\end{equation*}

Then we have that

\begin{equation*}
    \langle x, y \rangle = y^Tx=x^Ty
\end{equation*}

For instance, with this property we can show in a compact way that if $y\in \mathbb{R}^n$, $A\in \mathbb{R}^{n\times d}$, and $\theta\in \mathbb{R}^d$, then

\begin{equation*}
    \langle y, A\theta\rangle = \langle A^Ty, \theta\rangle
\end{equation*}

Indeed, 
\begin{equation*}
    \begin{aligned}
	\langle y, A\theta\rangle &= (A\theta)^Ty\\
	&= \theta^TA^Ty\\
	&=\theta^T(A^Ty)\\
	&=\langle \theta, A^Ty\rangle
    \end{aligned}
\end{equation*}

\subsection{\large\color{MidnightBlue}Statistics and Probability theory}

\subsubsection{\large\color{Periwinkle}Expected value, variance}

\begin{definition}{Moments of a distribution}

    Let $X$ be a real random variabe, and $k\in \mathbb{N}^*$. $X$ is said to have a moment of order $k$ if $E(|X|^k)<+\infty$, which means that:
    \begin{itemize}
	\item if $X$ is discrete, with image $X(\Omega)=(x_i)_{i\in \mathbb{N} }$, the series

	    \begin{equation*}
		\sum (x_i)^kP(X=x_i)
	    \end{equation*}

	    is \textbf{{absolutely}}  convergent. The moment is then equal to the sum of that series (without absolute value).
	\item is $X$ is continuous with density $p(x)$, the integral
	    \begin{equation*}
	        \int^{+\infty}_{-\infty} x^kf(x)  dx 
	    \end{equation*}
	    is \textbf{{absolutely}}  convergent. The moment is then equal to the sum of that series (without absolute value).
    \end{itemize}
\end{definition}

\begin{proposition}

    Let $k_1<k_2$ be integers. Let $X$ be a real random variable. Then if $X$ has a moment of order $k_2$, $X$ also has a moment of order $k_1$.
    
\end{proposition}

\begin{proof}
    We will prove the result for a discrete random variable $X$, but the proof is the same for a continuous random variable.

    Let us denote $(x_i)_{i\in \mathbb{N}}\in \mathbb{R}^{\mathbb{N}}$ the values taken by $X$, with probabilities $p_i$. By definition, the series $\sum |x_i|^{k_2}p_i$ is convergent. We also know that the series $\sum p_i$ is convergent, by definition of a probability distribution. Hence, the series $\sum p_i+ |x_i|^{k_2}p_i$ is convergent as well.

    But we remark that for any $\alpha\in \mathbb{R}$, $|\alpha|^{k_1}\leq 1 + |\alpha|^{k_2}$, by considering separately $|\alpha|\leq 1$ or $|\alpha|\geq 1$. Applying this result to $\alpha=x_i$ and multiplying by $p_i$, we have that $\forall i$:

    \begin{equation}
	p_i|x_i|^{k_1}\leq p_i+p_i|x_i|^{k_2}
    \end{equation}

    Which prooves that the series $\sum |x_i|^{k_1}p_i$ is convergent, which means that $X$ has a moment of order $k_1$.
    \\

    Alternatively, we could state that the variable $1+|X|^{k_2}$ has a moment of order $1$, and that by comparison $|X|^{k_1}\leq 1+|X|^{k_2}$ also has a moment of order $1$, which leads to the same conclusion.
	
\end{proof}

\begin{definition}{Expected value, variance}

    \begin{itemize}
        \item If $X$ has a moment of order $1$, it is called the \textbf{{expected value}} 
	\item If $X$ has a moment of order $2$, then $X-E(X)$ also has a moment of order $2$. This moment is called the variance of $X$.
	    \begin{equation}
		V(X)=E\big((X-E(X))^2\big)
	    \end{equation}
	    We often note $\sigma(X)= \sqrt{Var(X)}$. 
    \end{itemize}

\end{definition}

\begin{proposition}
    \label{prop:var}

    Let $a$ and $b$ be real numbers, and $X$ a random variable that admits a moment of order $2$. Then
    \begin{equation}
	Var(aX+b)=a^2Var(X)
    \end{equation}

\end{proposition}

\begin{proposition}
    Let $X$ and $Y$ be real two random variables that admit a moment of order $2$. Then the variable $XY$ has a moment of order $1$, and we then define the covariance of $X$ and $Y$ as
    \begin{equation*}
	Cov(X,Y) = E\big( (X-E(X))(Y-E(Y))\big)
    \end{equation*}
\end{proposition}
    

\begin{lemma}
    Let $X, Y, Z\in \mathbb{R} $ be real random variables with a moment of order $2$. We have:
\begin{equation*}
    Cov(X+Y, Z) = Cov(X,Z)+Cov(Y,Z)
\end{equation*}

\begin{equation*}
    Var(X+Y) = Var(X)+Var(Y)+2Cov(X,Y)
\end{equation*}

\begin{equation*}
    |Cov(X,Y)|\leq \sigma(X)\sigma(Y)
\end{equation*}
\end{lemma}

\begin{proposition}
    Let $(X_1, \dots, X_n)$ be $n$ mutually \textbf{{independent}}  real random variables. Then if they all admit a moment of order $1$, then the product $X_1X_2\dots X_n$ also does admit a moment of order $1$ and 
    \begin{equation}
	E(X_1X_2\dots X_n) = \prod^{n}_{i=1} E(X_i)
    \end{equation}
    If they also admit moments of order $2$, then
    
    \begin{equation}
	Var( \sum^{n}_{i=1} X_i ) = \sum^{n}_{i=1} Var(X_i)
    \end{equation}
\end{proposition}

\begin{definition}{Expected value for vectors, variance matrix}

    From now on, if we write $E(X)$ or $Var(X)$, we implicitely assume that the quantities are correctly defined. Let $X\in \mathbb{R}^d$ be a random vector.
\begin{equation*}
X=\begin{pmatrix}
X_{1}\\
...\\
X_{i}\\
...\\
X_{d}
\end{pmatrix}
\end{equation*}

The \textbf{{expected value}} of the vector writes
\begin{equation*}
    E(X)=\begin{pmatrix}
    E[X_{1}]\\
...\\
    E[X_{i}]\\
...\\
    E[X_{d}]
\end{pmatrix}
\end{equation*}

Note that we could similarly define the expected value of a matrix.

    The \textbf{{variance matrix}} (or \textbf{{covariance matrix, variance-covariance, dispersion matrix}})  $Var(X)$ is defined as

    \begin{equation*}
	[Var(X)]_{ij} = Cov(X_i, X_j)       
    \end{equation*}
\end{definition}

\begin{remark}
    It is useful to note that

    \begin{equation}
	Var(X) = E\Big[ (X-E[X])(X-E[X])^T\Big]
    \end{equation}
\end{remark}

\begin{lemma}
    Let $X \in \mathbb{R}^{d, p}$ be a random vector.

    If $A\in \mathbb{R}^{n,d}$ is a fixed matrix, $AX\in \mathbb{R}^{n,p}$ is a random vector and we have

    \begin{equation*}
	E(AX)=AE(X)
    \end{equation*}

    and 

        \begin{equation*}
	Var(AX) = AVar(X)A^T\in \mathbb{R}^{n, n}
    \end{equation*}
    
    Similarly, if $B\in \mathbb{R}^{p, m}$ is a fixed matrix, $XB \in \mathbb{R}^{d, m}$ is a random vector and we have
    \begin{equation*}
	E(XB)=E(X)B
    \end{equation*}
\end{lemma}

\begin{definition}{Independent and identically distributed variables}

    We say that the random variables $(X_n)_{n\in \mathbb{N} }$ are independent and identically distributed if $\forall i\in \mathbb{N} $, the law of $X_i$ is the same as the lax of $X_1$ and they are mutually independent. This is noted \textbf{{i.i.d variables.}} 
\end{definition}

\begin{remark}
    In machine learning, when studying a sequence of variables, each one representing for instance a sample drawn from a distribution, we most often assume that the resulting sequence is i.i.d.
\end{remark}


\subsubsection{\large\color{Periwinkle}Sample mean, sample variance, sample covariance}
\label{sec:empstats}

We observe a dataset $ \{x_i\in \mathbb{R}^d, i\in [1, n]\}$ of $n$ i.i.d samples. We assume that the underlying distribution has a moment of order $1$, noted $m\in \mathbb{R}^d$.

\begin{definition}{Sample mean vector}
    \begin{equation*}
	\overline{x} = \frac{1}{n} \sum^{n}_{i=1} x_i\in \mathbb{R}^d
    \end{equation*}
\end{definition}

\begin{remark}
The sample mean vector is an \textbf{{unbiased estimator}} of the expected value.

\begin{equation}
    E( \overline{x}) = m
\end{equation}

\url{https://en.wikipedia.org/wiki/Bias_(statistics)}
\end{remark}

\begin{definition}{Unbiased sample variance}

    If $d=1$, we define the \textbf{{unbiased sample variance}} by

    \begin{equation*}
	s^2= \frac{1}{n-1}  \sum^{n}_{i=1} (x_i- \overline{x})^2
    \end{equation*}
\end{definition}

\begin{remark}
    
    We assume that the underlying distribution has a moment of order $2$, noted $\sigma^2$. Then the unbiased sample variance is an \textbf{{unbiased estimator}} of the variance.

    \begin{equation*}
	E(s^2)=\sigma^2
    \end{equation*}

    If we replace the $ \frac{1}{n-1} $ factor by $ \frac{1}{n} $, we have a biased estimator. The use of $ \frac{1}{n-1} $ is called \textbf{{Bessel's correction}}. 
\end{remark}

\begin{definition}{Sample covariance matrix}

    We go back to the case of $ \{x_i\in \mathbb{R}^d, i\in [1, n]\}$. Each sample $x_i$ writes

\begin{equation*}
    x_i=\begin{pmatrix}
	x_{i1}\\
...\\
	x_{ij}\\
...\\
	x_{id}
\end{pmatrix}
\end{equation*}

    The sample covariance matrix is a $d\times d$ martix $S^2$ such that for all $(p,q)$

    \begin{equation*}
	S_{pq} = \frac{1}{n-1} \sum^{n}_{i=1} (x_{ip}- \bar{x}_p) (x_{iq}- \bar{x}_q) 
    \end{equation*}

    This can also be written

    \begin{equation*}
	S = \frac{1}{n-1} \sum^{n}_{i=1} (x_i- \bar{x})(x_i- \bar{x})^T
    \end{equation*}
\end{definition}

\begin{remark}
    The sample covariance matrix can also be written in a different way. We consider the \textbf{{sample matrix}} $X\in \mathbb{R}^{n,d}$.

    \begin{equation*}
    X=\begin{pmatrix}
	x_1^T\\
...\\
	x_i^T\\
...\\
	x_n^T
\end{pmatrix}
    \end{equation*}

    We can also consider the centered sample matrix defined by

    \begin{equation*}
	X_{\text{centered}}=\begin{pmatrix}
	x_1^T- \bar{x}^T\\
...\\
	x_i^T- \bar{x}^T\\
...\\
	x_n^T- \bar{x}^T
\end{pmatrix}
    \end{equation*}

    Then, the \textbf{{centered}} sample covariance matrix writes:

    \begin{equation*}
	S^2 = \frac{1}{n-1} X_{\text{centered}}^TX_{\text{centered}}
    \end{equation*}
\end{remark}

\subsubsection{\large\color{Periwinkle}Markov and Chebychev inequality}

\begin{proposition}{Markov inequality}

    Let $X$ ba a real non-negative random variable (variable al√©atoire r√©elle positive), such that $E(|X|)<+\infty$. Let $a>0$. Then

    \begin{equation}
	P(X\geq a)\leq \frac{E(X)}{a} 
    \end{equation}
    
\end{proposition}

\begin{proposition}{Chebyshev inequality}
    Let $X$ ba a real random variable, such that $E(|X|^2)<+\infty$. Let $a>0$. Then

    \begin{equation}
	P(|X-E[X]|>a)\leq \frac{Var(X)}{a^2} 
    \end{equation}
\end{proposition}


\begin{theorem}{Weak law of large numbers}

    Let $(X_i)_{i\in \mathbb{N} }$ be a sequence of i.i.d. variables that have a moment of order $2$. We note $m$ their expected value. Then

    \begin{equation}
	\forall \epsilon > 0, \lim_{n\rightarrow+\infty}P\big( |\frac{1}{n} \sum^{n}_{i=1} X_i-m|\geq \epsilon \big)= 0
    \end{equation}

We say that we have \textbf{{convergence in probability}}.
\end{theorem}

\begin{proof}

    Let $S_n= \frac{1}{n} \sum^{n}_{i=1} X_i$. By linearity of the expected value, $E(S_n)=m$. We note $\sigma^2=Var(X_1)$. By independence of the sequence $(X_n)_{n\in \mathbb{N} }$, we have that $Var(S_n)= \sum^{n}_{i=1} Var( \frac{1}{n}  X_i)$, which is equal to $ \frac{1}{n^2} \sum^{n}_{i=1} Var(X_i)= \frac{\sigma^2}{n} $ with equation \ref{prop:var}.

    Using the Chebychev inequality, 

    \begin{equation*}
        \begin{aligned}
	    P(|S_n-m|>\epsilon) &\leq \frac{Var(S_n)}{\epsilon^2} \\
	    &= \frac{\sigma^2}{n\epsilon^2} 
        \end{aligned}
    \end{equation*}
    
\end{proof}

\begin{remark}
    Keeping the same hypotheses on the sequence of variables, we can make some important observations.
    \begin{itemize}
	\item $\lim_{n\rightarrow +\infty}Var(S_n)=0$.
	\item More precisely, we have a probabilistic bound on the magnitude of the difference between the average $S_n$ and $m$.

	    \begin{equation}
		\sqrt{Var\Big( S_n-m \Big)} = \frac{\sigma}{ \sqrt{n} } 
	    \end{equation}
    \end{itemize}
\end{remark}

The following theorem gives a result that applies to a different type of 
convergence of random variables (convergence in distribution). As convergence
in probability implies convergence in distribution, we say that convergence in
distribution is a \textbf{{weaker}} convergence.

\begin{theorem}{Central limit theorem}
    Let $(X_n)_{n\in \mathbb{N} }$ be a sequence of i.i.d. variables that have
    a moment of order $2$. We note $m$ their expected value, and $ \sigma^2$
    their variance. If we consider the following random variable:

    \begin{equation}
        T_n = \frac{ \sqrt{n} }{\sigma} (S_n-m)
    \end{equation}

    then $T_n$ converges in distribution to $ \mathcal{N}(0, 1)$.
\end{theorem}


\subsubsection{\large\color{Periwinkle}Concentration inequalities}

Concentration inequalities are useful in order to have \textbf{{non-asymptotic }} bounds on the deviation between the average of random variables and their mean. Non-asymptotic means that we have an inequality for all $n$, as opposed to the central limit theorem that is asymptotic and holds for $n\rightarrow+\infty$.

\begin{theorem}{Bernstein inequality}
Let $u_1, \dots, u_n$ be i.i.d. random variables, such that $E(u)=0$ and $|u|\leq B$ almost surely, with $B>0$. Let $ \sigma^2=E(u^2)$ and $t>0$:

\begin{equation*}
    P( \frac{1}{n} \sum^{n}_{i=1} u_i >t)\leq e^{ - \frac{t^2n/2}{\sigma^2+bt} }
\end{equation*}
\end{theorem}

\begin{corollary}
    Under the same assumptions:
\begin{equation*}
    P( |\frac{1}{n} \sum^{n}_{i=1} u_i |>t)\leq 2e^{ - \frac{t^2n/2}{\sigma^2+bt} }
\end{equation*}
    
\end{corollary}

\begin{theorem}{Hoeffding's inequality}

    Let $(X_i)_{1\leq i\leq n}$ be $n$ i.i.d real random variables such that $\forall i\in [1, n]$, $X_i\in [a,b]$ and $ E(X_i)= \mu \in \mathbb{R} $.  Let $	\bar{\mu} = \frac{1}{n} \sum^{n}_{i=1} X_i$.

    Then $\forall \epsilon >0$,

    \begin{equation*}
	P\Big(| \bar{\mu}-\mu|\geq \epsilon \Big) \leq 2 \exp\Big( -\frac{2n\epsilon^2}{(b-a)^2} \Big)
    \end{equation*}
\end{theorem}

\subsection{\large\color{MidnightBlue}Differential calculus}

\subsubsection{\large\color{Periwinkle}Differentiable functions}

\begin{definition}{Differentiable function}
    \label{def:diff}

    Let $V$ and $W$ be real Hilbert spaces (complete vector space with an inner product). Let $f: V\rightarrow W $. We say that $f$ is differentiable in $x\in V$ if there exsists a continuous linear map $L_x:V\rightarrow \mathbb{R} $ such that

    \begin{equation*}
	f(x+h)=f(x)+L_x(h)+o(h)
    \end{equation*}

    with $ \lim_{h\rightarrow 0} \frac{|o(h)|}{||h||} =0$.
\end{definition}

\begin{remark}

    We consider the case (that we will often encounter) where $W= \mathbb{R}$. Then for a fiven $x\in B$, $L_x$ is a linear form, and since $V$ is a Hilbert space, with Riesz representation theorem we know that, there exists a unique vector $p_x\in V$ such that $\forall h\in V, L_x(h)= \langle p, h \rangle$. $p$ is sometimes noted $f'(x)$,  $ \nabla_xf$ or $\nabla f(x)$. We note that we do not assume here that $V$ is of finite dimension.
    
\end{remark}

\begin{definition}{Two times differentiable function}

    We keep the same hypotheses as in \ref{def:diff} and assume that $W= \mathbb{R}$. If the map $x\mapsto \nabla_xf $ is differentiable in $x$, the we say that $f$ is two times differentiable in $x$. In that case we note $f"(x)$ the second-order derivative, that satisfies:

    \begin{equation*}
	\nabla_{x+h}f=\nabla_xf+f"(x)(h)+o(h)
    \end{equation*}
\end{definition}

\begin{lemma}
    We keep the same hypotheses. Given $x$ and $h$, $f"(x)(h)$ is an element of $V$, that can also be identified to an element of its dual space $V^*$. With the notation $f"(x)(h, h')=f"(x)(h)(h')$, we can show that

    \begin{equation*}
	f(x+h)=f(x)+\nabla_xf(h)+ \frac{1}{2} f"(x)(h,h)+o(||h||^2)        
    \end{equation*}
\end{lemma}

\subsubsection{\large\color{Periwinkle}Jacobian matrix, gradient and Hessian}

In this paragraph, we consider the case when $V= \mathbb{R}^d$, which is the most frequent situation encountered in machine learning.
\\

\textbf{{Jacobian matrix:}} If $f: \mathbb{R}^d\rightarrow \mathbb{R}^p$ is differentiable on $ \mathbb{R}^d$ we note $L^f_x$ the differential in $x$. It is an map from $ \mathbb{R}^d$ to $ \mathbb{R}^p$. We can then equivalently consider its matrix, that can be noted also $L^f_x\in \mathbb{R}^{p, d}$. 

\begin{equation*}
    f(x+h)=f(x)+L^f_xh+o(h)
\end{equation*}

$L^f_x$ is called the \textbf{{Jacobian matrix}}  of $f$ in $x$.
\\

\textbf{{Link with the gradient}}: we note that if the function $f$ has real values ($p=1$), then 

\begin{equation*}
    \nabla_xf=(L^f_x)^T\in \mathbb{R}^{d, 1}
\end{equation*}

\textbf{{Jacobian of a product.}} 

If we now consider $g: \mathbb{R}^p\rightarrow \mathbb{R}^q$ another differentiable map, and note its matrix / differential in $f(x)$ as $L^g_{f(x)}\in \mathbb{R}^{q, p}$, we have:

\begin{equation*}
    \begin{aligned}
	(g\circ f)(x+h)&=g(f(x+h))\\
	&=g(f(x)+L^f_xh+o(h))\\
	&=g(f(x)+L^f_xh)+L^g_{f(x)}o(h)+o(o(h))\\
    \end{aligned}
\end{equation*}

We can show that $L^g_{f(x)}o(h)+o(o(h))=o(h)$. Hence, 

\begin{equation*}
    \begin{aligned}
	(g\circ f)(x+h)&=g(f(x)+L^f_xh)+o(h)\\
	&=g(f(x))+L^g_{f(x)}L^f_xh+o(L^f_xh)+o(h)\\
    \end{aligned}
\end{equation*}

We can also show that $o(L^f_xh)=o(h)$, for instance using the operator norm of $L^f_x$.

Finally, the Jacobian matrix $L^{g\circ f}_x$ of $g\circ f: \mathbb{R}^d\rightarrow \mathbb{R}^q$ in $x$ is the product of the jacobians.

\begin{equation}
    \label{eq:jacobprod}
    L^{g\circ f}_x=L^{g}_{f(x)}L^f_x\in \mathbb{R}^{q, d}
\end{equation}


\textbf{{Gradient of a product:}} 
With the same functions $g$ and $f$ and if $q=1$, we deduce from \ref{eq:jacobprod} that 

\begin{equation*}
    \begin{aligned}
	\nabla_x(g\circ f)&= (L^{g\circ f}_x)^T\\
	&=(L^{g}_{f(x)}L^f_x)^T\\
	&=(L^f_x)^T(L^{g}_{f(x)})^T\\
	&= (L^f_x)^T\nabla_{f(x)}g\in \mathbb{R}^{d, 1}
    \end{aligned}
\end{equation*}

\textbf{{Hessian}}: if $f: \mathbb{R}^d\rightarrow \mathbb{R} $ is two times differentiable in $x$, then the map $\nabla f: \mathbb{R}^d\rightarrow \mathbb{R}^d$, $x\mapsto\nabla_xf$ has a matrix $H^f_x\in \mathbb{R}^{d,d}$, called the Hessian.

\begin{equation*}
    \nabla_{x+h}f=\nabla_xf+H^f_xh+o(h)
\end{equation*}

Then, the development of $f$ around $x$ can be written

\begin{equation*}
    f(x+h)=f(x)+L^f_xh+ \frac{1}{2} h^T(H^f_x)h+o(||h||^2)
\end{equation*}

\textbf{{Examples}} 

\begin{itemize}
    \item[ \textbf{{Quadratic function}}] Let $A\in \mathbb{R}^{d,d}$ be a symmetric real matrix. If $f(x)= \frac{1}{2} x^TAx-b^Tx$, then
	\begin{itemize}
	    \item $\nabla_xf=Ax-b$ 
	    \item $H_x^f=A$.
	\end{itemize}
    \item[ \textbf{{Least squares}} ] Let $X\in \mathbb{R}^{n,d}$, $\theta\in \mathbb{R}^d$, $y\in \mathbb{R}^n$. If $f(\theta)= \frac{1}{2} ||X\theta-y||_2^2$, then
	\begin{itemize}
	    \item $\nabla_{\theta}f=X^T(X\theta-y)$
	    \item $H_{\theta}^f=X^TX$.
	\end{itemize}
\end{itemize}

\textbf{{Explicit formulation of the gradient, the Jacobian, the Hessian}}. We assume $f: \mathbb{R}^d\rightarrow  \mathbb{R}^p $ is differentiable.

\begin{equation*}
    \forall x\in \mathbb{R}^d, f(x)=
\begin{pmatrix}
    f_1(x)\\
...\\
    f_{i}(x)\\
...\\
    f_{d}(x)
\end{pmatrix}
\end{equation*}

\begin{equation*}
L^f_x=
\begin{pmatrix}
    \frac{\partial f_1}{\partial x_1}(x)  & \frac{\partial f_1}{\partial x_2}(x) & \dots & \frac{\partial f_1}{\partial x_d}(x) \\
\dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots \\
    \frac{\partial f_p}{\partial x_1}(x)  & \frac{\partial f_p}{\partial x_2}(x) & \dots & \frac{\partial f_p}{\partial x_d}(x) \\
\end{pmatrix}
\end{equation*}

If $f$ has real values ($p=1$), then
\begin{equation*}
    \nabla_xf=
    \begin{pmatrix}
	\frac{\partial f}{\partial x_1} (x)\\
...\\
	\frac{\partial f}{\partial x_i}(x) \\
...\\
	\frac{\partial f}{\partial x_d} (x)\\
\end{pmatrix}
\end{equation*}

and if we keep $p=1$ and assume that $f$ is two times differentiable, then the Hessian reads:

\begin{equation*}
H^f_x=
\begin{pmatrix}
    \frac{\partial^2 f}{\partial^2 x_1}(x)  & \frac{\partial^2 f}{\partial x_2\partial x_1}(x) & \dots & \frac{\partial^2 f}{\partial x_d\partial x_1}(x) \\
\dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots \\
    \frac{\partial^2 f}{\partial x_1\partial x_d}(x)  & \frac{\partial^2 f}{\partial x_2\partial x_d}(x) & \dots & \frac{\partial^2 f}{\partial x_d^2}(x) \\
\end{pmatrix}
\end{equation*}

\begin{remark}
    By Swhartz theorem, if $f$ is $C^2$, then the Hessian is symmetrical.
\end{remark}




\subsubsection{\large\color{Periwinkle}Lipshitz continuity}
\label{def:lipshitz}

\begin{definition}{L-Lipschitz continuous function}

    Let $f$ be a differentiable function and $L>0$. We say that $f$ is $L$-Lipschitz continuous if $\forall x,y\in \mathbb{R}^d$, 
    \begin{equation*}
	||f(x)-f(y)||\leq L||x-y||
    \end{equation*}
\end{definition}

\begin{definition}{L-Lipschitz continuous gradients}

    Let $f$ be a differentiable function and $L>0$. We say that $f$ has $L$-Lipschitz continuous gradients if $\forall x,y\in \mathbb{R}^d$, 
    \begin{equation*}
	||\nabla_xf-\nabla_yf||\leq L||x-y||
    \end{equation*}
\end{definition}


\subsubsection{\large\color{Periwinkle}Smoothness}

\begin{definition}{Smoothness}

    A differentiable function $f$ with real values is said $L$-smooth if and only if
    \begin{equation*}
	\forall x, y\in \mathbb{R}^d, |f(y)-f(x)-\nabla_xf(y-x)|\leq \frac{L}{2} ||y-x||^2
    \end{equation*}
\end{definition}

\begin{lemma}
    $f$ is $L$-smooth if and only if it has $L$-Lipshitz continuous gradients.
\end{lemma}

\begin{proof}
    We consider, for $x$ and $y$ fixed $\in \mathbb{R}^d$, the map:

$$
u  : \left\{
    \begin{array}{ll}
	t\rightarrow f\big(x+t(y-x)\big) & \\
        \mathbb{R}  \mapsto \mathbb{R} & 
    \end{array}
\right.
$$

    $u$ is differentiable in $ \mathbb{R} $ and $u'(t) = \langle\nabla_{x+t(y-x)}f, y-x \rangle$. Hence, 

    \begin{equation}
	\begin{aligned}
	    \label{eq:application}
	    f(y)-f(x) &= u(1)-u(0)\\
	    &= \int^{1}_{0} u'(t)  dt \\
	    &= \int^{1}_{0}  \langle\nabla_{x+t(y-x)}f, y-x \rangle dt \\
	\end{aligned}
    \end{equation}

    $\Rightarrow$ \textbf{{)}}  We assume that $f$ has $L$-Lipshitz continuous gradients.  For all $t\in [0, 1]$, we can write that

    \begin{equation}
	\label{eq:difference}
	\nabla_{x+t(y-x)}f = \nabla_{x}f +\Big(\nabla_{x+t(y-x)}f- \nabla_{x}f\Big) 
    \end{equation}

    Inserting \ref{eq:difference} in \ref{eq:application}, we get that

    \begin{equation}
	\begin{aligned}
	    \label{eq:}
	    f(y)-f(x) &= \int^{1}_{0} \langle\nabla_{x}f, y-x \rangle dt +\int^{1}_{0}  \langle(\nabla_{x+t(y-x)}f- \nabla_{x}f) , y-x \rangle   dt \\
	    &= \langle\nabla_{x}f, y-x \rangle +\int^{1}_{0}  \langle(\nabla_{x+t(y-x)}f- \nabla_{x}f) , y-x \rangle   dt \\
	\end{aligned}
    \end{equation}

    and that

    \begin{equation}
        \begin{aligned}
            \label{eq:}
	    |f(y)-f(x)-\langle\nabla_{x}f, y-x \rangle| &= |\int^{1}_{0}  \langle(\nabla_{x+t(y-x)}f- \nabla_{x}f) , y-x \rangle   dt |\\
	    &\leq  \int^{1}_{0}  ||\nabla_{x+t(y-x)}f- \nabla_{x}f||\times || y-x|| dt \\
	    &\leq  \int^{1}_{0}  L||t(y-x)||\times|| y-x|| dt \\
	    &\leq  ||y-x||^2\int^{1}_{0}  Ltdt\\
	    &= \frac{L}{2} ||y-x||^2
        \end{aligned}
    \end{equation}

\end{proof}

\begin{lemma}
    
    If $f$ is twice differentiable, $f$ is $L$-smooth if and only if
    \begin{equation*}
        \forall x\in \mathbb{R}^d, -LI_d\leq H_xf\leq LI_d
    \end{equation*}

	 Meaning that all eigenvalues of $H_xf$ have a module that is smaller than $L$. Here, $\geq$ denotes the \textbf{{Loewner order}}, which is a partial order on symmetric matrices. \url{https://en.wikipedia.org/wiki/Loewner_order}.
\end{lemma}

\subsection{\large\color{MidnightBlue}Algorithmic complexities}

In machine learning, there is a focus on the efficiency of algorithm, as large-scale problems are frequent and some algorithmic complexities are prohibitive. We give some reminders on orders of magnitudes of time-complexity of some operations, ignoring space complexity in this section.

\begin{itemize}
    \item If $A\in \mathbb{R}^{d, d}$ is invertible, the cost of inverting it is $ \mathcal{O} (d^3)$.
    \item If $A\in \mathbb{R}^{d, d}$ is symmetric, the cost of computing its eigenvalues and eigenvectors $ \mathcal{O} (d^3)$.
    \item If $X\in \mathbb{R}^{n,d}$ and $\theta\in \mathbb{R}^d$, the cost of computing $ X\theta\in \mathbb{R}^n$ is $ \mathcal{O} (nd)$.
\end{itemize}

Some optimisations of these operations exist, see:
\begin{itemize}
    \item QR decomposition for matrix inversion (\url{https://en.wikipedia.org/wiki/QR_decomposition}).
    \item Low-rank approximation. \url{https://en.wikipedia.org/wiki/Low-rank_approximation} 
\end{itemize}

\section{\large\color{Blue}Optimization}

Reference: \cite[]{Allaire2012} 

\subsection{\large\color{MidnightBlue}Definitions}

\begin{definition}{Minima}
    Let $f: \mathbb{R}^d\rightarrow \mathbb{R}$ be defined on $K\subset \mathbb{R}^d$.

    $x\in K$ is a local minimum of $f$ on $K$ if and only if
    \begin{equation*}
	\exists \delta>0, \forall y\in K, ||y-x||<\delta \Rightarrow f(x)\leq f(y)
    \end{equation*}

    $x\in K$ is a global minimum of $f$ on $K$ if and only if
    \begin{equation*}
	\forall y\in K, f(x)\leq f(y)
    \end{equation*}
\end{definition}

\begin{definition}{Coercive function}

    $f:V\rightarrow \mathbb{R} $, defined on a vector space, $V$ is \textbf{{coercive}} if 
    \begin{equation*}
	\lim_{||x||\rightarrow +\infty}f(x)=+\infty
    \end{equation*}
\end{definition}

\subsection{\large\color{MidnightBlue}Existence result}

\begin{theorem}{Existence of a global minimum in $\mathbb{R}^d$}
    \label{th:dimfinie}

    Let $K$ be a closed non-empty subset of $ \mathbb{R}^d$, and $f: \mathbb{R}^d\rightarrow \mathbb{R} $ a continuous coercive function. Then, there exists at least a global minimum of $f$ on $K$.
\end{theorem}

\subsection{\large\color{MidnightBlue}Convex analysis}

Convexity is a key property of function, that allows to have theoretical proofs on the existence of solutions to a optimization problem, and on the convergence of optimization algorithms. It is useful in finite dimensional spaces and also in infinite dimensional spaces.

\subsubsection{\large\color{Periwinkle}Definitions}

\begin{definition}{Convex function}

    The function $f:\Omega \rightarrow \mathbb{R} $ with $\Omega$ convex is:
    \begin{itemize}
	\item \textbf{{convex}} if $\forall x, y\in \Omega, \alpha\in [0,1]$,
	    \begin{equation*}
		f(\alpha x+(1-\alpha)y)\leq \alpha f(x)+(1-\alpha)f(y)
	    \end{equation*}
	\item \textbf{{strictly convex}} if $\forall x, y\in \Omega, \alpha\in [0,1]$,
	    \begin{equation*}
		f(\alpha x+(1-\alpha)y)< \alpha f(x)+(1-\alpha)f(y)
	    \end{equation*}
	\item $\mu$-\textbf{{strongly convex}} if $\forall x, y\in \Omega, \alpha\in [0,1]$,
	    \begin{equation*}
		f(\alpha x+(1-\alpha)y)\leq \alpha f(x)+(1-\alpha)f(y)- \frac{\mu}{2} \alpha(1-\alpha)||x-y||^2
	    \end{equation*}
    \end{itemize}
\end{definition}

\begin{lemma} Let $f$ be convex on convex set $ \Omega$, $f:\Omega \rightarrow \mathbb{R} $.  Let $c\in \mathbb{R} $. Then

\begin{equation*}
    \Omega_c = \{x\in \Omega, f(x)\leq c\}
\end{equation*}
 
 is a convex set.
\end{lemma}

\begin{proposition}
    $f$ is $ \mu$ convex if and only if $ x\mapsto f(x)- \frac{\mu}{2} ||x||^2$ is convex.
\end{proposition}

\textbf{{Examples:}} 

\begin{itemize}
    \item All norms are convex.
    \item $x\mapsto \log(1+e^{-x})$ is convex on $ \mathbb{R} $
    \item $x\mapsto \theta^Tx$ is convex on $ \mathbb{R}^d$ with $\theta \in \mathbb{R}^d$ (linear form)
    \item if $Q$ is a symmetric semidefinite positive matrix (matrice positive), then the positive quadratic form $x\mapsto x^TQx$ is convex.
    \item if $Q$ is a symmetric definite positive matrix (matrice d√©finie positive) with smallest eigenvalue $\lambda_{min}>0$, then the positive definite quadratic form $x\mapsto x^TQx$ is $ 2\lambda_{min}$- strongly convex.
    \item if $X\in \mathbb{R}^{n\times d}$, and $Y\in \mathbb{R}^n$, $\theta\mapsto ||X\theta-Y||^2$ is convex on $ \mathbb{R}^d$ (Least-squares)
    \item If $f$ is increasing and convex and $g$ is convex, then $f \circ g $ is convex.
    \item Is $f$ in convex and $g$ is linear, then $f \circ g $ is convex.
\end{itemize}

\begin{definition}{Condition number}

    If $f$ is $L$-smooth and $\mu$-strongly convex, we define its \textbf{{condition number}} as:

    \begin{equation*}
        \kappa = \frac{L}{\mu} 
    \end{equation*}

\end{definition}

\begin{remark}
    The condition number is an important quantity to consider when studying the convergence speed of gradient algorithms.
\end{remark}


\subsubsection{\large\color{Periwinkle}Differential formulation of convexity}

\begin{proposition}
    \label{prop:conv}
    Let $f:V \rightarrow \mathbb{R} $ be a differentiable function. The following conditions are equivalent.
    \begin{itemize}
        \item $f$ is convex.
	\item $\forall x, y\in V, f(y)\geq f(x)+(f'(x)|y-x)$ ($f$ is above its tangent space)
	\item $\forall x, y\in V, (f'(x)-f'(y)|x-y)\geq 0$ ($f'$ grows)
    \end{itemize}
\end{proposition}

\begin{remark}
    Equivalently
    \begin{equation*}
	\forall u,v\in V, f(u)-f(v)\leq f'(u)(v-u)
    \end{equation*}
\end{remark}

\begin{proposition}
    \label{prop:alphaconv}
    Let $f:V \rightarrow \mathbb{R} $ be a differentiable function, and $\mu>0$. The following conditions are equivalent.
    \begin{itemize}
        \item $f$ is $ \mu$-convex
	\item $\forall x, y\in V, f(y)\geq f(x)+(f'(x)|y-x)+ \frac{  \mu}{2} ||y-x||^2$
	\item $\forall x, y\in V, (f'(x)-f'(y)|x-y)\geq \mu ||x-y||^2$
    \end{itemize}
\end{proposition}

\begin{lemma}{Lojasiewicz inequality}

    Let $f: \mathbb{R}^d\rightarrow \mathbb{R} $ be a $\mu$-strongly convex differentiable function with a unique minimiser $x^*$. Then, we have that for all $x\in \mathbb{R}^d$, 

    \begin{equation*}
	||\nabla_xf||_2^2\geq 2\mu \big(f(x)-f(x^*)\big)
    \end{equation*}
\end{lemma}

\begin{proposition}{Convexity of two-times differentiable functions}

    We now assume that $f$ is two times differentiable.

    \begin{itemize}
	\item $f$ is convex if anf only if
	    \begin{equation*}
		\forall x, h\in y, J"(x)(h, h)\geq 0
	    \end{equation*}
	\item $f$ is $\mu$-strongly convex if and only if
	    \begin{equation*}
		\forall x, h\in y, J"(x)(h, h)\geq \mu ||h||^2
	    \end{equation*}
    \end{itemize}

\end{proposition}

\begin{remark}
    In the case of $V= \mathbb{R}^d$, the two previous conditions mean respectively that 

	    \begin{equation}
		\label{eq:hessconv}
		\forall x, h\in y, h^T(H_xf)h\geq 0
	    \end{equation}

	    and

	    \begin{equation}
		\label{eq:hessconvmu}
		\forall x, h\in y, h^T(H_xf)h\geq \mu ||h||^2
	    \end{equation}

	    \ref{eq:hessconv} means that $\forall x\in \mathbb{R}^d$, all eigenvalues of $H_xf$ are non-negative, while \ref{eq:hessconvmu} means that they all are $\geq \mu$.



\end{remark}

\subsubsection{\large\color{Periwinkle}Minima of convex functions}

\begin{proposition}
    \label{prop:miniconv}

    \begin{itemize}
        \item If $f$ is convex, any local minimum is a global minimum. The set of global minimizers is a convex set.
	\item If $f$ is strictly convex, there exists at most one local minimum (that is thus global).
	\item If $f$ is convex and $C^1$ (differentiable, $a\mapsto df_a$ continuous), then $x$ is a minimum (thus global) of $f$ on $V$ if and only if the gradient cancels in $x$, $\nabla_xf=0$. $V$ need not be finite-dimensional.
    \end{itemize}
\end{proposition}

\subsubsection{\large\color{Periwinkle}Existence of a global minima for convex functions}

\begin{theorem}{Existence of a global minimum, strongly convex case}
    \label{th:alphaconv}

    Let $K$ be a closed non-empty subset of the Hilbert space $H$, and $f: \mathbb{R}^d\rightarrow \mathbb{R} $ a continuous $\mu$-strongly convex function. Then, there exists a unique a global minimum $u$ of $f$ on $K$, and

    \begin{equation*}
	\forall x\in K, ||x-u||^2\leq \frac{4}{\mu} [ f(x)-f(u)]
    \end{equation*}
\end{theorem}

\begin{theorem}{Existence of a global minimum, convex case}
    \label{th:alphaconv}

    Let $K$ be a closed non-empty subset of the Hilbert space $H$, and $f: \mathbb{R}^d\rightarrow \mathbb{R} $ a continuous convex coercive function. Then, there exists a global minimum $u$ of $f$ on $K$.

\end{theorem}

\subsubsection{\large\color{Periwinkle}Local minimum of a two-times differentiable function}

\begin{theorem}
    
We assume that $K=V$ ans $f$ is two-times diff√©rentiable in $x$.

If $x$ is a local minimizer of $f$, then
\begin{itemize}
    \item $f'(x)=0$
    \item $\forall h\in V, f"(u)(h,h)\geq 0$
\end{itemize}


\end{theorem}

\begin{remark}
    
    In this theorem, we do not assume that $f$ is convex.
\end{remark}

\begin{remark}
   When  $V= \mathbb{R}^d$, the previous conditions translate in the fact that $H_xf$ is positive semi-deminite or that $H_yf$ is positive semidefinite in a neighborhood of $x$. 
\end{remark}


\subsubsection{\large\color{Periwinkle}Expected values and convex functions}

Let $D$ be a convex set. If $f$ is convex, then if $p: S\rightarrow D$ is such that $p(x)\geq 0$ and $ \int_{S} p(x)  dx =1$, then

\begin{equation*}
    f( \int_{S} p(x)x  dx)\leq \int_{S} p(x)f(x)  dx
\end{equation*}

Let $D$ be a convex set. If $X$ is a random variable such that $ X\in D$ almost surely and $ E[X]$ exists, then

\begin{equation*}
    f(E[X])\leq E[f(X)]
\end{equation*}

\subsection{\large\color{MidnightBlue}Gradient Descent (GD)}

\subsubsection{\large\color{Periwinkle}Context}

GD is an important first-order algorithm for optimization. A first-order iterative algorithm only has access to the first-order derivative of the current iterate. For well-conditioned convex problems, it converges exponentially fast. We want to minimize a differentiable function $f$. The \textbf{{gradient descent (GD)}}  update reads:

\begin{equation*}
    x_{t+1} = x_t - \gamma_t \nabla f(x_t)
\end{equation*}


Any $x_t\in \mathbb{R}^d$ is called an \textbf{{iterate}} of the algorithm. GD is sometimes also called \textit{{determinisitic}} of \textit{{full
gradient}} descent, as opposed to Stochastic Gradient Descent (SGD, see \ref{sec:sgd}).

\subsubsection{\large\color{Periwinkle}Minimisation of a strongly convex function}

The following lemma shows that if the step size is small enough, then GD is a \textbf{{descent algorithm.}} 

\begin{lemma}
    let $L>0$, $f: \mathbb{R}^d\rightarrow \mathbb{r} $ a convex function with $L$-lipshitz continuous gradients (see \ref{def:lipshitz}). Let $x_0\in \mathbb{R}^d$, $\gamma_t >0$. Then

    \begin{equation*}
	f(x_t)\leq f(x_{t-1})-\gamma_t (1-L\gamma_t) ||\nabla f(x_{t-1})||^2
    \end{equation*}

    In particular if $0<  \gamma_t < \frac{1}{L} $, then $ \forall t\in \mathbb{N} $ such that $ x_{t-1}$ is not a global minimum,

    \begin{equation*}
	f(x_t)<f(x_{t-1})
    \end{equation*}

    Otherwise $x_t = x_{t-1}$ and $f(x_t) = f(x_{t-1})$.
\end{lemma}

\begin{theorem}{Convergence of GD for a strongly convex function}

    Let $f: \mathbb{R}^d\rightarrow \mathbb{R}	$ be a  $\mu$-strongly convex and $L$-smooth ($L$ Lipshitz continuous gradients) function. Let $x^*$ be the global minimum of $f$ (which we know exists since $f$ is strongly convex), $x_0\in \mathbb{R} $, $T\in \mathbb{N}$.

    \begin{itemize}
	\item With constant step size $\gamma_t = \frac{1}{L} $, we have

	    \begin{equation*}
		f(x_t)-f(x^*)\leq ( 1- \frac{\mu}{L})^t\big(f(x_0)-f(x^*)\big) 
	    \end{equation*}
        \item With constant step size $0<\gamma < \frac{1}{2l} $, we have

    \begin{equation}
	\label{eq:gdconv}
	||x_t-x^*||^2\leq (1-\gamma \mu)^t ||x_0-x^*||^2
    \end{equation}
    \end{itemize}
\end{theorem}

\begin{remark}
    \ref{eq:gdconv} could seem surprising at first sight, since it implies that $ (1-\mu\gamma)\geq 0$ if $0<\gamma< \frac{1}{2l}$. However, we have
\begin{equation*}
    \mu ||x-y||^2\leq \langle \nabla_xf-\nabla_yf, x-y \rangle \leq L||x-y||^2
\end{equation*}

Hence, $\mu\leq L$, so $\mu\gamma < \frac{1}{2} $.
\end{remark}

\begin{corollary}
    We note that $ (1-\gamma \mu)^T = (1- \frac{ \gamma \mu T}{T} )^T\leq e^{-\gamma \mu T}$. Thus,

    \begin{equation*}
	||x_t-t^*||^2\leq e^{-\gamma\mu T}||x_0-x^*||^2
    \end{equation*}

    We call this an \textbf{{exponential convergence}}. It is also sometimes called \textbf{{linear convergence}}. We can deduce that if 
    \begin{equation*}
	T \geq \frac{1}{ \gamma \mu} \log \big( \frac{||x_0-x^*||^2}{\epsilon}  \big)
    \end{equation*}
    then

    \begin{equation*}
        ||x_T-x^*||\leq \epsilon
    \end{equation*}
\end{corollary}

\subsubsection{\large\color{Periwinkle}Minimization of a convex function}

\begin{theorem}{Convergence of GD for a smooth convex function}

    Let $f: \mathbb{R}^d\rightarrow \mathbb{R} $ be a convex and $L$-smooth ($L$ Lipshitz continuous gradients) function, with global minimiser $x^*$.
    With constant step-size $\gamma_t = \frac{1}{L} $, the iterates $x_t$ of GD satisfy:

    \begin{equation*}
	f(x_t)-f(x^*)\leq \frac{L}{2t} ||x_0-\eta^*||^2
    \end{equation*}
\end{theorem}


\section{\large\color{Blue}Supervised Learning}

\subsection{\large\color{MidnightBlue}General definitions and bounds}

\subsubsection{\large\color{Periwinkle}Setup}

Given some \textbf{{observations}} $(x_i,y_i)\in \mathcal{X} \times \mathcal{Y}$ of input/output pairs, we would like to predict well the new output $y\in \mathcal{Y} $ that should be associated with a new input $x\in \mathcal{X} $. The training dataset is noted $D_n = \{(x_i, y_i), i\in [1, \dots, n]\}$.

We assume that there exists an \textbf{{unknown distribution}} $\rho$ for the joint variable $(X,Y)$.  In most theoretical frameworks used to study Supervised Learning, each sample $(x_i,y_i)$ is assumed to be drawn independently from this law. Hence, $D_n$ is a \textbf{{random variable.}} 

\begin{equation}
    (x_i,y_i)\sim \rho, \forall i\in[1, n]
\end{equation}

A \textbf{{learning rule}} $ \mathcal{A} $ is a map ("application" in french) that associates a \textbf{{prediction function}}, or \textbf{{estimator}}  $ \tilde{f}_n$, to $D_n$.

$$
\mathcal{A}  : \left\{
    \begin{array}{ll}
	\cup_{n\in \mathbb{N} } ( \mathcal{X} \times \mathcal{Y} )^n\rightarrow \mathcal{Y}^{ \mathcal{X} } & \\
        D_n \mapsto \tilde{f}_n& 
    \end{array}
\right.
$$

Since $D_n$ is a random variable, and since $ \tilde{f}_n$ depends on $D_n$, $ \tilde{f}_n$ is also a random variable.

\subsubsection{\large\color{Periwinkle}Loss functions}

\begin{definition}{Loss function}

    A \textbf{{loss function}} $l$ is a map that measures the discrepancy between to elements of a set (for instance of a linear space).
$$
l: \left\{
    \begin{array}{ll}
	\mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}_+ & \\
	(y,y') \mapsto l(y,y')& 
    \end{array}
\right.
$$

\end{definition}

\textbf{{Examples:}} The most common loss functions are the following:
\begin{itemize}
    \item["0-1" loss] for \textbf{{classification.}} $ \mathcal{Y}  = \{0, 1\}$ or $ \mathcal{Y}  = \{-1, 1\}$.

	\begin{equation}
	    l(y,z) = 1_{y\neq z}
	\end{equation}
    \item[square loss] for \textbf{{regression}}. $ \mathcal{Y}  = \mathbb{R} $. 

	\begin{equation}
	    l(y,z) = (y-z)^2
	\end{equation}
    \item[absolute loss] for \textbf{{regression}}. $ \mathcal{Y}  = \mathbb{R} $. 

	\begin{equation}
	    l(y,z) = |y-z|
	\end{equation}
\end{itemize}

In section \ref{subsec:logistic}, we will introduce other loss functions, that are necessary to solve classification problems.

\subsubsection{\large\color{Periwinkle}Risks}

\begin{definition}{Empirical distribution}

    We note $\rho_n$ the empirical distribution of the data, that represents the drawn dataset.

    \begin{equation}
	\rho_n = \frac{1}{n} \sum^{n}_{i=1} \delta_{(x_i, y_i)}
    \end{equation}

    where $\delta_{(x_i, y_i)}$ is the Dirac mass in $(x_i, y_i)$ (\url{https://fr.wikipedia.org/wiki/Mesure_de_Dirac}).
\end{definition}

\begin{definition}{Risks}

    Let $l$ be a loss. The \textbf{{risk}} (or \textbf{{statistical risk, generalization error, test error)}} of estimator $f$ writes

    \begin{equation}
	R(f)=E_{(X,Y)\sim \rho}[l(Y,f(X))]
    \end{equation}

    The \textbf{{empirical risk (ER)}} of an estimator $f$ writes

    \begin{equation}
	R_n(f) = \frac{1}{n} \sum^{n}_{i=1} l(y_i, f(x_i))
    \end{equation}

    We emphasize that the risks depends on the loss $l$.
\end{definition}

\begin{remark}

    The empirical risk can be expressed with the empirical distribution.

    \begin{equation}
        R_n(f)=E_{(X,Y)\sim \rho_n}[l(Y,f(X))]
    \end{equation}
\end{remark}

We define the \textbf{{target function}} $f^*$ by

\begin{equation}
    f^*\in \argmin_{f:X\rightarrow Y} R(f)
\end{equation}

with $f:X\rightarrow Y$ set of measurable functions.

\begin{remark}
    With the \textbf{{law of total expectation}}, we can write the risk as a the
    expectation of a conditional expectation.

    \begin{equation}
        \begin{aligned}
            \label{eq:}
            R(f) &= E_{(X,Y)\sim \rho}[l(Y,f(X))]\\
            &= E_{X}\Big[E[l(Y,f(X))|X]\Big]\\
        \end{aligned}
    \end{equation}

    $E\big(l(X,Y)|X \big) $ is the expectation of $l(X, Y)$ given $X$.

    \url{https://en.wikipedia.org/wiki/Law_of_total_expectation} 
\end{remark}


\begin{definition}{Fundamental problem of Supervised Learning}

    Estimate $f^*$ given only $D_n$ and $l$.
    
\end{definition}

\begin{definition}{Excess risk}

    The \textbf{{excess risk}} $ \mathcal{R} ( \tilde{f}_n)$ measures how close  $ \tilde{f}_n$ (predictor obtained by ERM) is to the best possible $f^*$, in terms of expected risk (generalization error) on new examples.

    \begin{equation}
	\mathcal{R} ( \tilde{f}_n) = R( \tilde{f}_n)-R(f^*)
    \end{equation}

    We also define the \textbf{{excess probability}} as
    \begin{equation*}
	P\big( R( \tilde{f}_n)-R(f^*)>t\big), \forall t>0
    \end{equation*}
\end{definition}

\begin{remark}
Since $ \tilde{f}_n$ is a random variable, $R_n( \tilde{f}_n)$ and thus $ \mathcal{R} (\tilde{f}_n)$ are also random variables.
\end{remark}


\begin{definition}{Consistency}

    The algorithm $\mathcal{A} $ is said to be \textbf{{consistent}}  if 

    \begin{equation*}
	\lim_{n\rightarrow +\infty} E_{D_n} \mathcal{R} ( \tilde{f}_n)=0 
    \end{equation*}
\end{definition}

    We also say that $ \mathcal{A} $ is a \textbf{{learning algorithm.}} 

\begin{definition}{Strong consistency}

    The algorithm $\mathcal{A} $ is said to be \textbf{{strongly consistent}}  if with probability $1$

    \begin{equation*}
	\lim_{n\rightarrow +\infty} \mathcal{R} ( \tilde{f}_n)=0 
    \end{equation*}


\end{definition}

\begin{definition}{Learning Rates}

    The sequence $(e_n)_{n \in \mathbb{R} } $ is a learning rate in expectation if

    \begin{equation*}
	E_{D_n} \big[\mathcal{R} ( \tilde{f}_n)\big]\leq e_n, \forall n\in \mathbb{N} 
    \end{equation*}
    
\end{definition}

\begin{definition}{Learning rate in probability}

    Let $\delta \in ]0, 1]$.  The  sequence $(p_{n, \delta})_{n\in \mathbb{N} }$ is a learning rate in probability if

    \begin{equation*}
	P_{D_n}( \mathcal{R} ( \tilde{f}_n)\geq \delta)\leq p_{n,\delta}, \forall n\in \mathbb{N} 
    \end{equation*}
\end{definition}


    \subsubsection{\large\color{Periwinkle}Bayes predictor and conditional expectation}

    Under some conditions, that are most often satisfied in a machine learning
    context, we can give an explicit formulation of $f^*$, the best predictor
    in $ \mathcal{Y}^{ \mathcal{X} }$, although we can not compute it without
    the knowledge of the distribution of $(X,Y)$. In this paragraph we ignore
    the measurability issues that were mentioned earlier, as it is often not an
    important issue for machine learning problems.

    \begin{theorem}{Bayes predictor}
        
	If $ \mathcal{Y} $ is compact and $l$ is continuous, we have

	\begin{equation}
	    \label{eq:bayes}
	    f^*(x) = \argmin_{y\in \mathcal{Y} }E\big[ l(Y, y)|X=x\big]
	\end{equation}

	almost everywhere. $E[q(Y)|X=x]$ denotes the conditional expectation of $q(Y)$ given that $X=x$, with $ q:Y\mapsto \mathbb{R} $. $f^*$ is also called the \textbf{{Bayes}} predictor.
    \end{theorem}

    \begin{definition}{Bayes risk}

	The Bayes risk is the risk of the Bayes predictor.

	\begin{equation}
        \begin{aligned}
            \label{eq:}
	    R^* &= E_X\Big[ \inf_{y\in \mathcal{Y} }E_Y\big[ l(Y, y)|X\big] \Big]\\
            %&= E_X\Big( E\big( l(Y, f^*(X))|X\big) \Big)\\
        \end{aligned}
	\end{equation}
    \end{definition}


    \begin{remark}
	\begin{itemize}
	    \item $Y$ is a random variable while the parameter $y$ in \ref{eq:bayes} is a value.
	    \item As we do not know the joint distribution $\rho$, it is not possible to compute the Bayes predictor from this expression.
	    \item Here, each possible value $x$  taken by $X$ is considered independently, as we do not require any regularity in $f^*: \mathcal{X}\rightarrow \mathcal{Y} $.
	\end{itemize}
    \end{remark}


    \subsubsection{\large\color{Periwinkle}Bayes estimator for regression and squared loss}

    In that case, we can be even more precise on the value of the Bayes predictor. We assume that $ \mathcal{X} = \mathbb{R}^d$ and $ \mathcal{Y}= \mathbb{R} $, and that we use the square loss $l(y,z)=(y-z)^2$. Hence, for each $x\in \mathcal{X} $,

\begin{equation*}
    f^*(x)= \argmin_{y\in \mathcal{Y} }E\Big[(y-Y)^2 |X=x\Big]
\end{equation*}

We can develop this expression 

\begin{equation*}
    \begin{aligned}
	E\Big[(y-Y)^2 |X=x\Big]&= E\Big[\big(y-E(Y|X=x)+E(Y|X=x)-Y\big)^2 |X=x\Big]\\
	&=E\Big[\big(y-E(Y|X=x)\big)^2+2\big(y-E(Y|X=x)\big)\big(E(Y|X=x)-Y\big)+\big(E(Y|X=x)-Y\big)^2 |X=x\Big]\\
    \end{aligned}
\end{equation*}

However, given $y$, $(y-E(Y|X=x))^2$  is a fixed scalar, and not a random variable, so 

\begin{equation*}
    E\Big[\big(y-E(Y|X=x)\big)^2\Big]=\big(y-E(Y|X=x)\big)^2
\end{equation*}

We also have that
\begin{equation*}
    E\Big[2\big(y-E(Y|X=x)\big)\big(E(Y|X=x)-Y\big)|X=x\Big]=2\big(y-E(Y|X=x)\big)E\Big[\big(E(Y|X=x)-Y\big)|X=x\Big]=0
\end{equation*}

As a consequence, the value that minimizes $E\big[(y-Y)^2 |X=x\big]$ is
\begin{equation}
    f^*(x)=E\big[Y|X=x\big]
\end{equation}

\begin{remark}
    As stated before, remember that in order to be able to compute $E\big[Y|X=x\big]$, we need the conditional law
    $P(Y|X=x)$, or equivalently  the joint law $P(X,Y)$ \textbf{{and}} the prior
    distribution $P(X)$. But again, in practical situations this is not the
    case.
\end{remark}

\subsubsection{\large\color{Periwinkle}Bayes predictor for classification and "0-1" loss}

In this situation, we can also express the Bayes predictor $f^*(x)$ as a
function of the probabilities. This time, $ \mathcal{X} = \mathbb{R}^d$ and $
\mathcal{Y}  = \{-1, 1\}$ or $ \mathcal{Y}  = \{0, 1\}$ (we use the latter
setting). We use the "0-1" loss $l(y,z) = 1_{y\neq z}$. In this setting, given a fixed value $X=x$ and a prediction $z\in \{0, 1\}$,
the computation of $E\Big[l(Y,z)|X=x\Big]$ is
straightforward.

\begin{equation}
    \begin{aligned}
        \label{eq:}
        E\Big[l(Y,z)|X=x\Big] &= 1 \times P(Y\neq z|X=x)+0\times P(Y=z|X=x)\\
        &=  P(Y\neq z|X=x)
    \end{aligned}
\end{equation}


Now, we have

    \begin{equation}
        \begin{aligned}
            \label{eq:}
	    f^*(x)&= \argmin_{z\in \mathcal{Y} }E\Big[l(Y,z)|X=x\Big]\\
	    &= \argmin_{z\in \mathcal{Y} }P(Y\neq z|X=x)\\
	    &= \argmin_{z\in \mathcal{Y} }1-P(Y=z|X=x)\\
	    &= \argmax_{z\in \mathcal{Y} }P(Y=z|X=x)
        \end{aligned}
    \end{equation}

    The optimal classifier selects the most probable output given $X=x$. We can
    now formulate the Bayes risk more explicitely, as a function of the
    probabilities. We note $\eta(x) = P(Y=1|X=x)$. Then, 
    \begin{itemize}
        \item If $ \eta(x)> \frac{1}{2} $, then $f^*(x)=1$, and $P\big( Y\neq f^*(x)\big) =   P\big( Y=0\big)=1-\eta(x)$
	\item If $ \eta(x)< \frac{1}{2} $, then $f^*(x)=0$, and $P\big( Y\neq f^*(x)\big) =   P\big( Y=1\big)=\eta(x)$
    \end{itemize}

    In both cases, $P\big( Y\neq f^*(x)\big)= \min(\eta(x), 1-\eta(x))$.
    \\

    Finally,

    \begin{equation}
	R^* = E_X\Big[ \min(\eta(X), 1-\eta(X))\Big]
    \end{equation}

\subsubsection{\large\color{Periwinkle}Bias-Variance decomposition}

Several methods and conventions exist in order to perform \textbf{{risk decompositions}}. All these decompositions aim at making a distinction between different, often antagonist contributions to the excess risk. We remember that as $ \tilde{f}_n$ is a random variable, $ R(\tilde{f}_n)$ is also a random variable. One possible risk decomposition is the following one.

\begin{equation*}
    E\big[R( \tilde{f}_n)\big]-R^* = \Big(E\big[R( \tilde{f}_n)\big]- \inf_{f\in F}R(f)\Big) +\Big(\inf_{f\in F}R(f)-R^* \Big)
\end{equation*}

We note that both terms are positive.

\begin{itemize}
    \item \textbf{{Approximation error (bias term)}}: depends on $f^*$ and $F$, not on $ \tilde{f}_n$, $D_n$.
	\begin{equation*}
	    \inf_{f\in F}R(f)-R^*
	\end{equation*}
	It is sometimes also defined as $\inf_{f\in F}R(f)$.
    \item \textbf{{Estimation error (variance term, fluctuation error, stochastic error)}}: depends on $D_n$, $F$, $ \tilde{f}_n$. To bound this term we need assumptions on the distribution $P$.
	\begin{equation*}
	    E\big[R( \tilde{f}_n)\big]- \inf_{f\in F}R(f)
	\end{equation*}
\end{itemize}


When the algorithm is \textbf{{underfitting}}, the capacity of the hypothesis space is too small, the functions are not able to approximate $f^*$ well and the variance is small. On the contraty, if $F$ is too large, there likely exists a function $f\in F$ close to $f^*$, inducing a small bias, but the variance might be large (\textbf{{overfitting}}).

\begin{remark}

\begin{definition}{Best estimator in hypothesis space}

    It is defined as
    \begin{equation*}
	f_a = \argmin_{f\in F}R(h)
    \end{equation*}
\end{definition}

    Then the bias-variance decomposition can also be written as
\begin{equation*}
    E[R( \tilde{f}_n)-R^*] = \Big(E(R( \tilde{f}_n))- R(f_a)\Big) +\Big(R(f_a)-R^* \Big)
\end{equation*}
\end{remark}

\subsubsection{\large\color{Periwinkle}Expected value of empirical risk}

For some \textbf{{fixed}} $h\in H$, $R_n(h)$ is a random variable, as it depends on the dataset $D_n$. Let us show that $ R_n(h)$ is an \textbf{{unbiased estimator}} of the real risk $R$. We assume that the samples $(X_i, Y_i)$ are i.i.d, with the distribution of $(X,Y)$, noted $\rho$. 

    \begin{equation*}
	    \begin{aligned}
		E_{D_n\sim \rho}(R_n(h)) &= \frac{1}{n} \sum^{n}_{i=1} E_{D_n\sim \rho}(l(h(X_i), Y_i)) \\
		&= \frac{1}{n} \sum^{n}_{i=1} E_{(X,Y)\sim \rho}(l(h(X), Y)) \\
		&=  E_{(X,Y)\sim \rho}(l(h(X), Y)) \\
		&= R(h)
	    \end{aligned}
    \end{equation*}

    However, this is not true for $ \tilde{f}_n$, as $ \tilde{f}_n$ depends on the training dataset $ D_n$. We cannot say that for all $i\in [1, n]$, $E_{D_n\sim \rho}(l( \tilde{f}_n(X_i), Y_i)) = E_{(X,Y)\sim\rho}(l( \tilde{f}_n(X), Y))$.
    \\

    For instance, let us consider the following situation. $X$ follows a uniform law on $]0, 1]$, and $Y=3X+\sigma\epsilon$, with $\epsilon$ being a standard Gaussian random variable independent from $X$, hence $ \mathcal{X} = ]0, 1]$ and $ \mathcal{Y} = \mathbb{R} $. We assume that $n=1$, and $D_1 = \{(x_1, y_1)\}=\{(\alpha, \beta)\}$. We assume that $F$, the space of available functions for $ \tilde{f}_n$, is the space of linear functions, of the form $ x\mapsto ax$, with $a\in \mathbb{R}$. If our learning rule is empirical risk minimization, with quadratic loss, then the estimator $ \tilde{f}_n$ writes $ \tilde{f}_n(x)= \frac{\beta}{\alpha}x$, and $l( \tilde{f}_n(\alpha), \beta)=0$. Hence,

    \begin{equation}
	E_{D_1\sim \rho}(l( \tilde{f}_n(X_1), Y_1))=0
    \end{equation}

    However, we can show that $E_{(X,Y)\sim \rho}(l( \tilde{f}_n(X), Y))\neq 0$. Indeed,

    \begin{equation*}
        \begin{aligned}
            \label{eq:}
	    E_{(X,Y)}(l( \tilde{f}_n(X), Y))&=E_{(X,\epsilon)}(l( \tilde{f}_n(X), 3X+\sigma\epsilon))\\
            &=E_{(X,\epsilon)}\big(( (\frac{\beta}{\alpha} - 3)X-\sigma\epsilon)^2\big)\\
            &\neq 0
        \end{aligned}
    \end{equation*}

    %\begin{equation*}
    %    \begin{aligned}
    %        \label{eq:}
    %        E_{(X,Y)\sim \rho}(l( \tilde{f}_n(X), Y))&= E_{(X,Y)\sim \rho}((\tilde{f}_n(X)- Y)^2)
    %    \end{aligned}
    %\end{equation*}

    \subsubsection{\large\color{Periwinkle}Deterministic bound on the estimation error}

We recall that we note $f_a$ the minimizer of the generalization error in the
considered space of functions.

    \begin{equation}
        f_a = \argmin_{h\in F}R(h)
    \end{equation}

    In this pragraph, we also note $f_n$ the minimizer of the empirical risk.

    \begin{equation}
        f_n = \argmin_{h\in F}R_n(h)
    \end{equation}

    \begin{proposition}
	\label{prop:dilemma}
	\begin{equation}
	    R(f_a)\leq R( \tilde{f}_n)\leq R(f_a)+2 \sup_{f\in F}|R(f)- R_n(f)|
	\end{equation}
    \end{proposition}

    \begin{remark}
        
	This can be written as
	\begin{equation}
	    R( \tilde{f}_n)- R(f_a)\leq 2 \sup_{f\in F}|R(f)- R_n(f)|
	\end{equation}
    \end{remark}

    \begin{proof}

        First, by definition of $f_a$, we know that since $f_n\in F$,

        \begin{equation}
            R(f_a)\leq R(f_n) 
        \end{equation}

        The inequality at the right comes from writing $R(f_n)-R(f_a)$ in the
        following way

    \begin{equation}
        \begin{aligned}
	    R(f_n)-R(f_a) &= \big(R(f_n)-R_n( f_n)\big)\\
	    &+\big(R_n( f_n)- R_n(f_a)\big)\\
	    &+\big(R_n(f_a)-R(f_a)\big)
        \end{aligned}
    \end{equation}

    \begin{itemize}
        \item By definition of $f_n$, we have that

            \begin{equation}
                R_n( f_n)- R_n(f_a) \leq 0
            \end{equation}
        \item Since $f_n\in F$, $R(f_n)-R_n( f_n)\leq \sup_{f\in F}|R(f)- R_n(f)| $
        \item Since $f_a\in F$, $R(f_a)-R_n( f_a)\leq \sup_{f\in F}|R(f)- R_a(f)| $
    \end{itemize}

        Which allows to conclude.
    \end{proof}

    \subsubsection{\large\color{Periwinkle}Scoring}

    \url{https://scikit-learn.org/stable/modules/model_evaluation.html} 

\subsection{\large\color{MidnightBlue}Probably Approximately Correct Learning: PAC bounds}

\subsubsection{\large\color{Periwinkle}Definition}

\begin{definition}{PAC accuracy}

    We say that $ \tilde{f}_n$ is $\epsilon$-accurate with confidence $1-\delta$ or $(\epsilon, \delta)-PAC$ if

    \begin{equation*}
	P_{D_n}\Big(R( \tilde{f}_n)- \inf_{f\in F}R(f)> \epsilon \Big)<\delta
    \end{equation*}

    \begin{remark}
         Equivalently:
	 \begin{equation*}
	P_{D_n}\Big(R( \tilde{f}_n)- \inf_{f\in F}R(f)\leq  \epsilon \Big)\geq 1-\delta
	 \end{equation*}
    \end{remark}
\end{definition}


    \subsubsection{\large\color{Periwinkle}Bound in probability on the fluctuation error}

    \begin{theorem}

	\textbf{{Assumptions:}} 
	\begin{itemize}
	    \item $\exists B>0, \forall y,y'\in Y, l(y,y')\leq B$, Hence, $\forall f\in F, R(f)\leq B$.
	    \item $ F $ is finite of cardinal $|F|$.
	\end{itemize}

	Let $\delta>0$. If $n\geq \frac{2|F|}{\delta} $, we have:


%\begin{equation*}
%    \epsilon^2 = \frac{ \log(|F|)+\log ( \frac{2}{ \delta}) }{2n} 
%\end{equation*}

	\begin{equation*}
	    P\Big( R( \tilde{f}_n)-\inf_{f\in F}(R(f))> \sqrt{ \frac{4B^2 \log( \frac{2|F|}{\delta} )}{n} }  \Big)\leq \delta 
	\end{equation*}
    \end{theorem}

    \begin{corollary}
        
	With probability $ 1-\delta$, we have

	\begin{equation}
	    R( \tilde{f})\leq R(f_a)+2 \sqrt{ \frac{ \log(|F|)+ \log( \frac{2}{\delta} )}{2n} } 
	\end{equation}
    \end{corollary}

    \begin{remark}
	\begin{itemize}
	    \item We say that the estimation error is $ \mathcal{O} ( \frac{1}{ \sqrt{n} } )$.
	    \item This bound does not depend on de law $p$ from which the samples are drawn. Thus, it is a bound that is \textbf{{uniform}} with regards to the law of $ (X,Y)$.
	    \item $\log(|F|)$ can be seen as the entropy of the class $F$, if the probability distribution if uniform on that class.
	    \item theoretically, $F$ is often an \textbf{{infinite set.}} However, it can be discretized. If $F$ has $m$ parameters quantified on $N$ values, then $ |F| = N^m$, hence $\log(|F|)=m\log(N)$. In order to control the term $ \frac{\log(|F|)}{2n} $, then we should have $ n >> m$.
	    \item To have results for infinite sets, the Vapnik-Chervonenkis theory is required.
	\end{itemize}
    \end{remark}

    \subsubsection{\large\color{Periwinkle}Probabilistic bound on the generalization error for a decaying approximation error}

    If we add a hypothesis on the \textbf{{decay}} of the approximation error $ R(f_a)$, as a function of $|F|$, we can bound \textbf{{the generelisation error.}} 

    \begin{proposition}
	\label{prop:decay_approx}

	Assume $\exists \beta , C >0$ such that

	\begin{equation}
	    \label{eq:decay}
	    R(f_a)\leq C\big( \log(|F|) \big)^{-\beta}
	\end{equation}

	$\forall \epsilon >0$, if we have

	\begin{equation*}
	    n\geq C^{ \frac{1}{ \beta} } \epsilon^{-2- \frac{1}{\beta} }
	\end{equation*}

	then,

	\begin{equation*}
	    P(R( \tilde{f}_n)\leq 3\epsilon)\geq 1-2e^{ - C^{ \frac{1}{\beta} }\epsilon^{- \frac{1}{\beta} }}
	\end{equation*}
        
    \end{proposition}

    \subsubsection{\large\color{Periwinkle}Approximation error bound}

    \textbf{{Noiseless model:}} We introduce an \textit{{a priori}} knowledge in the problem, stating that there exists a (unknown) function $f$ linking the inputs $x$ to the outputs, without ambiguity. Thus, we can write $y=f(x)$.

    \textbf{{Classes of functions:}} We also assume $f$ belongs to a given class of functions $F$. Usually, $F$ determines the \textbf{{regularity }} of $f$ (differentiability order, Lipshitz, etc.).

    \begin{equation*}
	R(f_a)\leq \max_{f\in F}\min_{h\in H}||f-h||_{\infty}
    \end{equation*}

    Thus, if we have bounds such as
    \begin{equation}
	\label{eq:unif_bound}
\max_{f\in F}\min_{h\in H}||f-h||_{\infty}\leq C\big( \log(|F|) \big)^{-\beta}  
    \end{equation}

    we have \ref{eq:decay} and we can apply \ref{prop:decay_approx}.

    \subsubsection{\large\color{Periwinkle}Curse of dimensionality}

    We look for a bound of the type of \ref{eq:unif_bound}, where $F$ is the class of uniformly Lipshitz continuous ($\alpha=1$), and when  $h$ is a nearest neighbor algorithm.

    We show that in order to have $ \sup_{f\in F}||f- \tilde{f}||_{\infty}\leq \epsilon$, we must have

    \begin{equation*}
	n\geq \frac{\epsilon^{-d}d^{ \frac{d}{2} }}{(2\pi e)^{ \frac{d}{2} } }
    \end{equation*}

    \begin{remark}
        With $\alpha$ larger, the result is similar.
    \end{remark}


\subsection{\large\color{MidnightBlue}Ordinary Least Squares (OLS)}
\label{subsec:ols}

\subsubsection{\large\color{Periwinkle}Context}
\label{con:ols}

The OLS is an important supervised learning problem. $ \mathcal{X} = \mathbb{R}^d$, $ \mathcal{Y}  = \mathbb{R} $, and the estimator is a \textbf{{linear function}} parametrized by $ \theta\in \mathbb{R}^d$.

\begin{equation*}
    F = \{ x\mapsto x^T \theta, \theta \in \mathbb{R}^d\}
\end{equation*}

In a least squares problem, the loss $l$ writes 

\begin{equation*}
    l(y,y') = (y-y')^2
\end{equation*}

The dataset is stored in the \textbf{{design matrix}} $X\in \mathbb{R}^{n\times d}$.

\begin{equation*}
X=
\begin{pmatrix}
x_{1}^T\\
...\\
x_{i}^T\\
...\\
x_{n}^T
\end{pmatrix}
=
\begin{pmatrix}
    x_{11}, \:\dots\:,\: x_{1j}\:, \:\dots\: x_{1d}\\
...\\
    x_{i1}, \:\dots\:,\: x_{ij}\:, \:\dots\: x_{id}\\
...\\
    x_{n1}, \:\dots\:,\: x_{nj}\:, \:\dots\: x_{nd}\\
\end{pmatrix}
\end{equation*}

The vector of predictions of the estimator writes $Y = X\theta$.  Hence,

\begin{equation*}
    \begin{aligned}
	R_n(\theta) &= \frac{1}{n} \sum^{n}_{i=1} (y_i-\theta^Tx_i)^2\\
	&= \frac{1}{n} ||Y-X\theta||_2^2
    \end{aligned}
\end{equation*}

\begin{remark}
    A \textbf{{linear model}}, such as the OLS, can often be interpreted as
    predicting an output value (dependent variable) from combining the contributions from the $d$
    \textbf{{features}} (independent variables) of the input data, in a linear
    way. This can be useful for classification as well as regression.
\end{remark}

\subsubsection{\large\color{Periwinkle}OLS estimator}

We assume that $X$ is \textbf{{injective.}}  Necessary, $d\leq n$.

\begin{proposition}{Closed form solution}
    
    If $X$ is injective, there exists a unique minimiser of $R_n(\theta)$, called the \textbf{{OLS estimator}}, given by

    \begin{equation}
	\label{def:ols}
	\hat{ \theta}= (X^TX)^{-1}X^TY
    \end{equation}
\end{proposition}

\begin{remark}

    We consider the non-centered empirical covariance matrix:

    \begin{equation}
	\label{eq:Sigma}
	\hat{\Sigma} = \frac{1}{n} X^TX\in \mathbb{R}^{d, d}
    \end{equation}

    Then $ \hat{\Sigma}$ is invertible and

    \begin{equation}
	\hat{\theta} = \frac{1}{n} \hat{\Sigma}^{-1} X^TY
    \end{equation}
    
\end{remark}

\begin{proof}

    $R_n(\theta)$ is coercive, continuous, and in a finite dimensional space. Hence, it admits at least a minimizer, according to theorem \ref{th:dimfinie}.  It is also differentiable and convex, so the minimizers are the points cancelling the gradient.

    Let $u:x\mapsto ||u||_2^2$. If $du_x$ is de differential of $u$ in $x$, for all $x\in \mathbb{R}^d$, we have

    \begin{equation*}
	du_x(h) = 2\langle x,h\rangle
    \end{equation*}

    Let $dR_n(\theta)$ be the differential of $R_n$ in $\theta$.

    \begin{equation*}
	\begin{aligned}
	    dR_n(\theta)(h) &= \frac{1}{n} du_{Y-X\theta}(-Xh)\\
	    &= \frac{2}{n} \langle Y-X\theta, -Xh \rangle\\
	    &= \frac{2}{n} \langle X^T(X\theta-Y), h \rangle
	\end{aligned}
    \end{equation*}

    where we have used \ref{ssec:linalg}. We deduce that the gradient of $R_n$ in $\theta$ writes
    \begin{equation}
	\label{eq:olseq}
        \nabla_{\theta} R_n = \frac{2}{n} (X^TX\theta-X^TY)
    \end{equation}

    Let us show that $X^TX$ is \textbf{{inversible.}} As it is a square matrix $\in \mathbb{R}^{d\times d}$, we only need to show that it is injective. Let $u\in \mathbb{R}^d$, such that $X^TXu = 0$. Then $ \langle X^TXu, u \rangle=0$, but this inner product equals $\langle Xu, Xu \rangle = ||Xu||^2$. Hence $Xu=0$, but as $X$ is injective, $u=0$, which shows that $X^TX$ is injective.

    We conclude that the only solution to \ref{eq:olseq} is 

    \begin{equation*}
	\hat{\theta} = (X^TX)^{-1}X^TY
    \end{equation*}


% \textbf{{First proof:}} 
% 
% \begin{equation*}
%     \tilde{R_n}(w) = \frac{1}{n} \sum^{n}_{i=1} (Y_i - w^TX_i)^2
% \end{equation*}
% 
% On remarque que le co√ªt est convexe en $w$, puisque c'est une fonction convexe d'une application lin√©aire.
% 
% Notons $ X = [X_1, \dots, X_n]^T\in \mathbb{R}^{n\times d}$. Le risque empirique est nul si et seulement si 
% 
% \begin{equation*}
%     \label{ols:min}
%     Xw=Y
% \end{equation*}
% 
% Alors n√©cessairement
% \begin{equation*}
%     X^TXw-X^TY = 0
% \end{equation*}
% 
% Supposons que $X$ soit injective. On remarque que c'est une application de $ \mathbb{R}^d$ vers $ \mathbb{R}^n$, et que c'est donc possible seulement si $ d\leq n$. Montrons qu'alors $X^TX: \mathbb{R}^d\rightarrow \mathbb{R}^d$ est \textbf{{inversible}}.
% 
% En effet soit $u\in \mathbb{R}^d$, tel que $ X^TXu=0_{ \mathbb{R} ^d}$. Donc $ (X^TXu|u)=(Xu|Xu)=||Xu||^2=0$. Donc par injectivit√© $u=0$.
% 
% Ainsi, \textbf{{n√©cessairement}} 
% 
% \begin{equation*}
%     w = (X^TX)^{-1}X^TY
% \end{equation*}
% 
% Il y a donc une solution unique dans ce cas.

\end{proof}

\begin{remark}

    We deduce that $ \hat{Y} = X \hat{\theta} = X(X^TX)^{-1}X^TY$ is the orthogonal projection of $Y$ on the linear subspace $ Im(X)$. Hence $P_X = X(X^TX)^{-1}X^T$ is the projection matrix on $Im(X)$.
    
\end{remark}


\begin{remark}{Numerical resolution}

The cost of inverting $X^TX$ is $ \mathcal{O} (d^3)$ by the Gauss-Jordan method and is thus prohibitive in high dimensions (for instance $ > 10^5$).
\end{remark}


\subsubsection{\large\color{Periwinkle}Statistical analysis of OLS}

We are interested in quantitative guarantees on the generalization error of the OLS estimator. In order to compute such guarantees, it is necessary to make some assumptions about the way the data are generated.

\begin{definition}{Linear model}

    The assumption that there exists a vector $ \theta^*\in \mathbb{R}^d$ such that 

    \begin{equation*}
	y_i = \theta^{*T}x_i+z_i, \forall i\in [1, n]
    \end{equation*}
    and $Z_i$ is a centered noise (or error)  ($E[Z_i]=0$) with variance $ \sigma^2$, is called the \textbf{{linear model.}} 
\end{definition}

All expectations are then with respect to the law of $X$ and of $Z$.

\begin{remark}

    When $\sigma^2=0$, we are in the \textbf{{noiseless model.}} 
    
\end{remark}

\begin{definition}{Fixed design}

    In this setting, $X$ is assumed to be deterministic (fixed, hence the name), so the expectations are with respect to $z$ only. In this context, we define the \textbf{{fixed design risk}} 

    \begin{equation}
	\begin{aligned}
	    \label{eq:}
	    R_X(\theta)    &= E_y[ \frac{1}{n} \sum^{n}_{i=1} (y_i-\theta^Tx_i)^2]\\
	    &= E_y[ \frac{1}{n} ||y-X\theta||_2^2]
	\end{aligned}
    \end{equation}
    The index $X$ denotes that this quantity depends on the fixed input matrix $X$. For instance $ R_X(\theta^*) = \sigma^2$. 
\end{definition}

\begin{definition}{Random design}

    In this setting, $X$ is random, so the expectations are with respect to $X$ and $Z$.
\end{definition}

We will give statistical results only for the \textbf{{fixed design}} setting in the linear model. The fixed design setting is a \textbf{{simplifying assumption}}, that allows to have guarantees on the \textbf{{fixed design risk}} $R_X(\theta)$, not on the generalization error directly. The fixed design setting can also be seen as a method that allows the learning of the optimal prediction vector, given some inputs.

However, it is also possible to have similar results in the random design setting, but the proofs are harder.

\begin{proposition}{Risk decomposition}

    Let $||\theta||_{\Sigma}^2 = \theta^T\hat{\Sigma} \theta$ (it is called the \textit{{Mahalanobis distance norm}}).  Under the linear model with fixed design, for any fixed $\theta \in \mathbb{R}^d$, we have:
    \begin{equation}
	R_X(\theta)-R_X( \theta^*) = ||\theta-\theta^*||_{\hat{\Sigma}}^2
    \end{equation}

    and, if $\theta$ is a random variable,

    \begin{equation}
	E[R_X(\theta)]-R_X(\theta^*) = ||E[\theta]-\theta^*||_{\hat{\Sigma}}^2+E\Big[||\theta-E[\theta]||_{\hat{\Sigma}}^2\Big]
    \end{equation}
    \begin{itemize}
	\item $||E[\theta]-\theta^*||_{\hat{\Sigma}}^2$: Bias term
	\item $E\Big[||\theta-E[\theta]||_{\hat{\Sigma}}^2\Big]$: Variance term
    \end{itemize}
\end{proposition}

\begin{proposition}{Statistical property of OLS estimator}

    Under the same hypothesis (linear model, fixed design), the OLS estimator $ \hat{\theta}$ defined in \ref{def:ols} satisfies:

    \begin{itemize}
	\item $ \hat{\theta}$ is \textbf{{unbiased}} : $ E[ \hat{\theta}] = \theta^*$.
	\item $ Var( \hat{\theta}) = \frac{\sigma^2}{n} \hat{\Sigma}^{-1}$.
    \end{itemize}
\end{proposition}
    

$ \hat{\Sigma}^{-1}$ is often called the \textit{{precision matrix.}} 

\begin{corollary}{Distance to optimal parameter, excess risk of OLS}

    Still with the same hypothesis

    \begin{equation}
	Var(|| \hat{\theta}-\theta^*||^2) = \frac{d\sigma^2}{n} Tr(\hat{\Sigma}^{-1})
    \end{equation}

    and

    \begin{equation}
	E[R_X( \hat{\theta})]-R_X( \theta^*) = \frac{\sigma ^2d}{n} 
    \end{equation}

    We note that both these quantities increase linearly with the dimension.
\end{corollary}

This corollary is illustrated in figure \ref{fig:olsrisk}.

\begin{proposition}
    The expected value of the empirical risk of $ \hat{\theta}$ writes:

    \begin{equation}
	E[ R_n( \hat{\theta})] = \frac{n-d}{n} \sigma^2
    \end{equation}
\end{proposition}

\begin{proof}
Not stated here, the proof is obtained by direct calculations and matricial tricks.
\end{proof}

\begin{remark}
    In this expected value, both the dataset and $ \hat{\theta}$ are random variables.
\end{remark}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=1\linewidth]{ols_risks}
    \caption{Dependence of the excess risk on $n$ and $d$, in the setting used
    in TP1.}
    \label{fig:olsrisk}
\end{figure}

\subsection{\large\color{MidnightBlue}Ridge regression}

\subsubsection{\large\color{Periwinkle}Context}

When the dimension $d$ becomes larger (i.e. $ \frac{d}{n} $ approaches $1$ or becomes larger), several undesirable phenomena can appear:
\begin{itemize}
    \item the excess risk is of order $ \sigma^2$.
    \item the training data can be perfectly fit, and this often leads to a bad generalization performance.
    \item if $d>n$, $X^TX$ is not invertible and there is a linear subspace of points cancelling the gradient of the objective function. The problem is said to be \textbf{{poorly posed}} or \textbf{{unidentifiable.}}
\end{itemize}

\textbf{{Regularizing}} the problem is an approach to enforce the unicity of the solution at the cost of introducing a \textbf{{bias}} in the estimator. The unicity is guaranteed by the \textbf{{strong convexity}} of the new loss function.

\subsubsection{\large\color{Periwinkle}Ridge regression estimator}

\begin{definition}{Ridge regression estimator}

    It is defined as
\begin{equation}
    \label{risk:ridge}
    \hat{ \theta}_{\lambda} = \argmin_{\theta \in \mathbb{R}^d} \Big( \frac{1}{n} ||Y-X\theta||_2^2 + \lambda || \theta||_2^2 \Big)
\end{equation}
\end{definition}


\begin{proposition}

    The Ridge regression estimator is unique even if $X^TX$ is not inversible and is given by
    \begin{equation*}
	\hat{ \theta}_{\lambda} = \frac{1}{n} ( \hat{\Sigma} +\lambda I_d)^{-1}X^TY
    \end{equation*}
\end{proposition}

\begin{proof}
We can show that the loss \ref{risk:ridge} is strongly convex. To do so, it is sufficient to show that $x\mapsto ||\theta||^2$ is $2$-convex on $ \mathbb{R}^d$. We have
\begin{equation*}
    \forall u, v\in \mathbb{R}, ( \frac{u+v}{2} )^2= \frac{u^2}{2} + \frac{v^2}{2} - \frac{1}{4} (u-v)^2
\end{equation*}

    Hence in $\mathbb{R} $, $ u\mapsto u^2$ is $2$-convex. By summing linear mappings (the projections $\theta\mapsto \theta_i$) and $2$-convex maps ($\theta_i\mapsto \theta_i^2$), $\theta\mapsto ||\theta||^2$ is $2$-convex in $ \mathbb{R}^n$.

    By summation with $ \theta \mapsto \frac{1}{n} ||Y-X\theta||$, which is a convex function,  $\theta \mapsto \tilde{R}_n(\theta)$ is $2\lambda$-convex.

    Hence there exists a unique minimizer for $R_n$. As it is differentiable, the minimizer is uniquely defined by cancellation of the gradient. The gradient in $\theta$ writes
    \begin{equation*}
	\frac{2}{n} (X^TX\theta - X^Ty)+2\lambda \theta
    \end{equation*}
    The equation of the cancellation of the gradient is
    \begin{equation*}
	(\frac{2}{n} n \hat{\Sigma} + 2\lambda I_d)\theta_{\lambda} = \frac{2}{n} X^Ty
    \end{equation*}
    which we can write

    \begin{equation*}
	n(\hat{\Sigma}+\lambda I_d)\theta_{\lambda} = X^ty
    \end{equation*}

    $ \hat{\Sigma}+2\lambda I_d$ is a symmetric matrix with all eigenvalues $\geq 2\lambda$. Thus, it is invertible. Also, $\forall a\in \mathbb{R}^* $ and $ A\in GL_d\mathbb{R} $, $(aA)^{-1} = \frac{1}{a} A^{-1}$, which concludes the proof.
\end{proof}
    
\subsubsection{\large\color{Periwinkle}Statistical analysis of Ridge regression}

We can also analyse the statistical properties of the ridge estimator, with the same hypotheses as before: linear model, fixed design. It is also possible to do it in the random design and potentially infinite $d$.

\begin{proposition}
    
    Under the linear model assumption, with fixed design setting, the ridge regression estimator has the following excess risk

    \begin{equation}
	E[ R( \hat{\theta}_{\lambda}]-R^* = \lambda^2\theta^{*T} ( \hat{\Sigma}+\lambda I_d)^{-2} \hat{\Sigma} \theta^*+ \frac{\sigma^2}{n} tr[ \hat{\Sigma}^2( \hat{\Sigma}+\lambda I_d)^{-2} ]
    \end{equation}
\end{proposition}

\begin{proof}
As for OLS, the proof is obtained by direct calculation and matricial tricks.
\end{proof}


\textbf{{Comments:}} 
\begin{itemize}
    \item We observe again a bias / variance decomposition.
    \item We consider the bias term $B$:

        \begin{equation}
           B =  \lambda^2\theta^{*T} ( \hat{\Sigma}+\lambda I_d)^{-2} \hat{\Sigma} \theta^*
        \end{equation}
        \begin{itemize}
            \item The bias $B$ increases when $\lambda$ increases. It is an approximation error and does not depend on $n$.
            \item When $\lambda = 0$ and $ \hat{\Sigma}$ is invertible (which corresponds to OLS), $B=0$. 
            \item When $\lambda \rightarrow +\infty$, $B\rightarrow \theta^{*T} \hat{\Sigma}\theta^*$. 
        \end{itemize}

    \item We consider the variance term $V$:

        \begin{equation}
            V=\frac{\sigma^2}{n} tr[ \hat{\Sigma}^2( \hat{\Sigma}+\lambda I_d)^{-2} ]
        \end{equation}
        \begin{itemize}
            \item The variance $V$ decreases when $\lambda$ increases. It is an estimation error and depends on $n$
            \item When $\lambda = 0$ and $ \hat{\Sigma}$ is invertible (which corresponds to OLS), $V= \frac{\sigma^2d}{n} $.
            \item When $\lambda\rightarrow +\infty$, $V\rightarrow 0$. 
            \item When $n\rightarrow +\infty$, $V\rightarrow 0$. 
        \end{itemize}

    \item In most cases, it is preferrable to have a biased estimation ($\lambda >0$).
\end{itemize}

\subsubsection{\large\color{Periwinkle}Choice of $\lambda$}

A natural question is whether it is possible to have a lower excess risk with Ridge regression than with OLS, which means an excess risk smaller than $ \frac{\sigma^2d}{n} $.

\begin{proposition}
    With the choice 

    \begin{equation}
	\label{eq:lambdaopt}
	\lambda^* = \frac{\sigma \sqrt{tr( \hat{\Sigma})}}{||\theta^*||_2 \sqrt{n} } 
    \end{equation}
    then

    \begin{equation}
	\label{ref:ridgeopt}
	E[ R( \hat{\theta}_{\lambda}]-R^*\leq \frac{\sigma \sqrt{tr( \hat{\Sigma})}||\theta^*||_2 }{ \sqrt{n} } 
    \end{equation}
\end{proposition}

Thi proposition is illustrated in figure \ref{fig:ridge_risks_n=30}.

\begin{itemize}
    \item If the the norms of all $x_i$ are uniformly bounded by $R$, then we can observe by direct calculation that $tr( \Sigma)\leq R^2$, independently of $d$. If $R$ and $||\theta^*||_2$ remain finite, then $d$ plays no role in  the bound in \ref{ref:ridgeopt}. This is called a \textit{{dimension-free}} bound.
    \item The convergence to $0$ in OLS is in $ \frac{1}{n} $, while it is  in $ \frac{1}{ \sqrt{n} }$ for the ridge. However, for the ridge regression, the dependence in the noise if in $\sigma$, whereas it is $\sigma^2$ for OLS. Which one is preferrable will depend on the value of the constants, and will not necessary be the "fast" rate in $ \mathcal{O} ( \frac{1}{n} )$ (that is, the rate of OLS).
    \item In practical situations, the quantities involved in the computation
        of $\lambda^*$ in  \ref{eq:lambdaopt} are typically unknown. However
        this equation shows that there may exist a $\lambda$ with a good prediction performance, which can be found by cross validation.
\end{itemize}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=1\linewidth]{ridge_risks_n=30}
    \caption{Comparison of OLS and Ridge performance for a specific setting
    used in TP2.}
    \label{fig:ridge_risks_n=30}
\end{figure}

\subsection{\large\color{MidnightBlue}Lasso}

\subsubsection{\large\color{Periwinkle}Context}

The \textbf{{Lasso}}  is another regularization method for OLS. The original idea is to penalize the number of nonzero components of the found parameter $\theta$. However, if we were to do it using a penalization of the form $\lambda||\theta||_0$ ($||\theta||_0$ is precisely the number of nonzero components), this would lead to a non-convex optimization problem. Le Lasso replaces $||\theta||_0$ by $ ||\theta||_1$.

\subsubsection{\large\color{Periwinkle}LASSO estimator}

It is defined as

\begin{equation}
    \tilde{\theta}_{\lambda}\in \argmin_{\theta\in \mathbb{R}^d}\{||Y-X\theta||^2+\lambda ||\theta||_1\}
\end{equation}

\subsubsection{\large\color{Periwinkle}Numerical resolutions}

The problem is convex, so iterative first-order algorithms can be used with some efficiency. Popular methods include \textbf{{Coordinate desent, Fista, LARS}}.

\subsection{\large\color{MidnightBlue}Logistic regression}
\label{subsec:logistic}

Despite its name, \textbf{{logistic regression}} is a classification method. It
is thus sometimes called logistic classification.

\subsubsection{\large\color{Periwinkle}Context}

We want to perform a binary classification from $d$-dimensional inputs, using a
\textbf{{linear predictor}} $\hat{f}_{\theta}$, parametrized by $\theta\in \mathbb{R}^d$. The inputs are $x_i \in \mathbb{R}^d$, $1\leq i \leq n$ and the outputs: $y_i\in \{0, 1\}$. With this estimator, the decision function writes:

$$
\hat{y}_i = \hat{f}_{\theta}(x_i)=\left\{
    \begin{array}{ll}
	1 \text{ if } \theta^Tx_i \geq 0 & \\
        0 \text{ if } \theta^Tx_i < 0& 
    \end{array}
\right.
$$

\begin{remark}
    It is not always possible to separate the data in a linear way. Often we work with \textbf{{transformations}} of the data, $\phi(x)$ instead of $x$, where $\phi(x)$ is in a high dimensional space (Kernel methods).
\end{remark}


\subsubsection{\large\color{Periwinkle}Risk convexification}

\begin{definition}{Binary classification empirical risk}
    \begin{equation}
	\begin{aligned}
	    \label{eq:}
	    \hat{R}_n(\theta) &= \frac{1}{n} \sum^{n}_{i=1} \mathbb{1}_{y_i\neq \hat{f}_{\theta}(x_i)}    \\
	    &=\frac{1}{n} \sum^{n}_{i=1} l_{"0-1"}(y_i,\hat{f}_{\theta}(x_i))  
	\end{aligned}
    \end{equation}
\end{definition}

However it is hard to minimize this empirical risk as it is neither differentiable nor convex in $\theta$. We can replace it by a \textbf{{convex, differentiable surrogate loss}} (\textbf{{substitut convexe}}). Several possibilities exist instead of using $ \mathbb{1}_{y_i\neq \hat{f}(x_i)}    $ as $l$ (binary loss). The \textbf{{logistic loss}} and \textbf{{cross-enropy loss}} are frequently used options.

\begin{definition}{Cross-entropy loss}

    When $ \mathcal{Y}  = \{0, 1\}$, the cross-entropy loss can be used as a convex surrogate:
    \label{loss:cross}
	\begin{equation}
	    l(\hat{y}, y) = y\log(1+e^{- \hat{y}})+ (1-y)\log(1+e^{ \hat{y}})
	\end{equation}
\end{definition}

\begin{definition}

    When $Y\in \{-1, 1\}$, the logistic loss can be used as a convex surrogate: 
	\begin{equation}
	    \label{loss:logistic}
	    l(\hat{y}, y) = \log(1+e^{- \hat{y}\times y})
	\end{equation}
\end{definition}

\begin{remark}
    \begin{itemize}
	\item Sometimes the naming of these two losses is used interchangeably.
        \item The choice of $ \mathcal{Y} $ will depend on the context, the most convenient choice should be used.
	\item Another often used possibility is the \textbf{{hinge loss}}. It used the loss used for optimizing \textbf{{Support vector machines.}} 
    \end{itemize}
\end{remark}


\subsubsection{\large\color{Periwinkle}Logistic regression estimator}

\begin{definition}{Logistic regression estimator}
    \label{def:logicest}

    If $l$ is the logistic loss or the cross-entropy loss, it is defined as

    \begin{equation}
	\hat{\theta}_{logit} = \argmin_{\theta\in \mathbb{R}^d} \frac{1}{n} \sum^{n}_{i=1} l(x_i^T\theta, y_i) 
    \end{equation}
\end{definition}

\textbf{{Numerical resolution}} : The ER is differentiable and stricly convex and thus has a unique minimizer. However, the equation obtained by cancellation of the gradient has \textbf{{no closed form solution}}. To solve it, we need to use iterative algorithms such as gradient descent or Newton's method.

\textbf{{Regularization}}: To avoid overfitting, we may also add regularization to the logistic loss, for instance with a term $ \lambda || \theta||_2^2$.

\subsection{\large\color{MidnightBlue}Stochastic Gradient Descent (SGD)}
\label{sec:sgd}

\subsubsection{\large\color{Periwinkle}Context}

SGD is the standard optimization algorithm for large scale machine learning, as its computationnal complexity is lower than that of GD, while keeping good convergence speed. We want to minimise a function of the form

\begin{equation*}
    f(\theta)=  E_z L(\theta, z)
\end{equation*}

where 

\begin{equation*}
    E_z L(\theta, z) = \int_{\Omega} L(\theta, z)dp(z)
\end{equation*}


\begin{itemize}
    \item $z\in \Omega$, $\rho$ is a probability distribution over $ \Omega$.
    \item $L: \mathbb{R}^d\times \Omega \rightarrow \mathbb{R}$
    \item $\theta \in \mathbb{R}^d$ is the parameter we want to optimize.
    \item for instance $ z = (x_i, y_i)$ as in supervised learning and
\begin{equation*}
    L(\theta, z) = l(\theta^T x, y)
\end{equation*}
\end{itemize}

The idea of SGD is to use at each time $t$ an \textbf{{unbiased estimator}}
$g_t$  of the gradient of $f$, instead of computing the full gradient. Indeed,
for large problems (large $n$ and/or large $d$), the computation of the full gradient can be prohibitive computationally. Note that GD is
sometimes called \textit{{deterministic}} or \textit{{full gradient}}, as opposed to SGD.  
For large-scale machine learning, SGD and its variants are the most widely used
omtimization algorithms.
\\

\textbf{{1] Empirical risk minimisation (ERM)}}

For instance, $f$ can be the empirical risk $f_n$.

\begin{equation*}
    f_n(\theta) =  \frac{1}{n} \sum^{n}_{i=1} L(\theta, z_i)
\end{equation*}

By \textbf{{linearity}} of the expected value,

\begin{equation}
    \label{eq:sgdest}
    \nabla_{\theta}f_n = \frac{1}{n} \sum^{n}_{i=1} \nabla_{\theta}L(\theta, z_i)
\end{equation}

If at time $t$ we sample uniformly $i(t)\in \{1, \dots, n\}$, then if we define $g_t$ as

\begin{equation*}
    g_t = \nabla_{\theta}L(\theta, z_{i(t)})
\end{equation*}

then $g_t$ is an unbiased estimator of $\nabla_{\theta}f_n$. SGD uses $g_t$ instead of the \textbf{{batch gradient}} $\nabla_{\theta}f_n$.
\\

\textbf{{2] Expected risk minimization}} 

% We can go further: as $f_n(\theta)$ is an unbiased estimator of $f(\theta)$ over the sampled $D_n$, we have $E_{D_n}[f_n(\theta)] = E_z[f(\theta)]$. Hence, 
% 
% \begin{equation*}
%     \nabla_{\theta}E_{D_n}[f_n(\theta)] =\nabla_{\theta} E_z[f(\theta)]
% \end{equation*} 
% 
% We have that 
% 
% \begin{equation*}
%     \begin{aligned}
%         \label{eq:}
% 	\nabla_{\theta}E_{D_n}[f_n(\theta)] &= \nabla_{\theta}E_{D_n}[ \frac{1}{n} \sum^{n}_{i=1} L(\theta, z_i)] \\
% 	&= \nabla_{\theta}E[L(\theta, z)]
%     \end{aligned}
% \end{equation*}
% 
If the hypotheses of Leibniz theorem are satisfied (which is the case most of the time in Machine Learning), we can write

\begin{equation*}
    \begin{aligned}
	\nabla_{\theta}E[L(\theta, z)]&= \nabla_{\theta} \int_{\Omega} L(\theta, z) dz\\
	&= \int_{\Omega} \nabla_{\theta}L(\theta, z) dz\\
	&= E[\nabla_{\theta}L(\theta, z)]
    \end{aligned}
\end{equation*}

Hence, if $z\sim \rho$, $\nabla_{\theta}L(\theta, z)$ is an \textbf{{unbiased estimator}} of the gradient of the \textbf{{real risk.}} 

However, importantly, in real situations, we can not sample infinitely many $z\sim \rho$. We only have access to $z_{i_t}$, sampled from the finite dataset $D_n$, and thus $z_{i_t}\sim \hat{\rho}_n$.

\subsubsection{\large\color{Periwinkle}SGD updates}

The SGD update reads
\begin{equation*}
    \theta_t = \theta_{t-1}-\gamma_t \nabla_{\theta} L(\theta, z_{i_t})
\end{equation*}

where $z_{i_t}$ is the sample obtained at time $t$.

\subsubsection{\large\color{Periwinkle}SGD as an estimation of GD}

Let us compute the expected value of the SGD update over $z_{i_t}$ sampled from $\rho$, given a fixed learning rate $\gamma$, and given the previous iterate $ \theta_{t-1}$.

\begin{equation*}
    \begin{aligned}
	E_{z_{i_t}}\big[\theta_t\big] &= \theta_{t-1} - \gamma E_{z_{i_i}}\big[\nabla_{\theta_{t-1}} L(\theta_{t-1}, z_{i_t})\big]\\
	&= \theta_{t-1} - \gamma \nabla_{\theta} f(\theta_{t-1})
    \end{aligned}
\end{equation*}

Hence, in expectation, SGD behaves as GD.


\subsubsection{\large\color{Periwinkle}Convergence for a convex objective}

Reference: \cite{Bach2021} 

\begin{theorem}{Convergence of SGD for a convex function}
    \label{th:sgdconvex}

    We assume that:
    \begin{itemize}
	\item The gradients estimates are unbiased.
        \item $f$ is convex
	\item $f$ is $L$-Lipshitz continuous.
	\item The gradient is bounded: $\forall t$, $||g_t(\theta_{t-1})||_2^2\leq L^2$ almost surely.
	\item $f$ admits a minimiser $ \theta^*$ such that $||\theta^*-\theta_0||_2\leq D$.
    \end{itemize}

    Let $\gamma_t = \frac{D}{L \sqrt{t} } $, $\forall t\in \mathbb{N} $. Then, the iterates of SGD satisfy:

    \begin{equation}
	E\big[f( \hat{\theta}_t)-f(\theta^*)\big]\leq DL \frac{2+\log(t)}{2 \sqrt{t} }
    \end{equation}

    where 

    \begin{equation}
	\hat{\theta}_t = \frac{ \sum^{t}_{s=1} \gamma_s\theta_{s-1}}{ \sum^{t}_{s=1} \gamma_s} 
    \end{equation}

\end{theorem}

\subsubsection{\large\color{Periwinkle}Convergence for a strongly convex objective}

We replace $f$ by $g$ to obtain a strongly convex objective and consider the minimization of $u$.

\begin{equation*}
    u(\theta) = f(\theta)+ \frac{\mu}{2} ||\theta||_2^2
\end{equation*}

Now, the SGD iteration for $u$ reads:

\begin{equation}
    \label{eq:udate}
    \theta_t = \theta_{t-1}- \gamma_t\big(g_t(\theta_{t-1})+\mu \theta_{t-1}\big)
\end{equation}

\begin{theorem}{Convergence of a strongly convex objective}

    We use the same hypotheses as \ref{th:sgdconvex}, adding the assumption
    that $u$ admits a (necessary unique) mininizer $\theta^*$. Then, with
    $\gamma_t =  \frac{1}{\mu t} $ and the update \ref{eq:udate}, we have

    \begin{equation}
	E[g( \hat{\theta}_t)-g(\theta^*)]\leq \frac{2L^2 (1+\log t)}{\mu t} 
    \end{equation}

    where 

    \begin{equation}
	\hat{\theta}_t = \frac{1}{t} \sum^{t}_{s=1} \theta_{s-1}
    \end{equation}

\end{theorem}

\subsubsection{\large\color{Periwinkle}Convergence for a strongly convex L-smooth objective}

\begin{definition}{Variance of the estimator of the gradient}

    We define the \textbf{{variance}} of the estimator of the gradient as

    \begin{equation}
	\sigma^2(\theta) = E_{z}|| \nabla_{\theta} f(\theta)- \nabla_{\theta}L(\theta, z)||^2
    \end{equation}

    We note that $\sigma $ depends on $ \theta$.
    
\end{definition}

\begin{theorem}{Convergence of SGD for a strongly convex L-smooth function}
    
    We assume that:
    \begin{itemize}
	\item The gradients estimates are unbiased.
        \item $f$ is $\mu$ strongly convex.
	\item $f$ has $L$-Lipshitz continuous gradients
	\item $\exists \sigma^2>0, \forall \theta \in \mathbb{R}^d, \sigma^2(\theta)\leq \sigma^2$
    \end{itemize}
    Let $\gamma_t = \gamma$, $\forall t\in \mathbb{N} $ and $ 0<\gamma < \frac{1}{2L} $. Then

    \begin{equation}
	E_{z_1, \dots, z_T}||\theta_T-\theta^*||\leq (1-\mu\gamma)^T||\theta_0-\theta^*|| + \frac{ \gamma}{\mu} \sigma^2
    \end{equation}
\end{theorem}

\begin{remark}
    The $\log t$ factors at the numerator are often ignored when considering the computational complexities.
\end{remark}

\subsubsection{\large\color{Periwinkle}Computational complexity of GD and SGD}

Roughly speaking, in most situations the computational complexity of one
iteration of GD will be $n$ times that of one iteration of SGD. For instance, in the case of least-squares regression, computing a batch gradient (GD) costs $ \mathcal{O} (nd)$. Computing a SGD gradient estimate costs $ \mathcal{O} (d)$. In the case of Ridge regression, the loss is $\mu$-strongly convex and $L$-smooth, with a condition number $\kappa = \frac{L}{\mu} $. If $t$ denotes de number of iterations, then
\begin{itemize}
    \item GD has a convergence rate of $\mathcal{O} (\exp( - \frac{t}{\kappa} ))$. To get an error of $\epsilon$, we must have $ t= \mathcal{O} ( \kappa \log \frac{1}{\epsilon} )$. Since each iteration requires $ \mathcal{O} (nd)$ computations, the computation time will be $ \mathcal{O} (\kappa nd\log\frac{1}{\epsilon}) $.
    \item SGD has a convergence rate of $ \mathcal{O} ( \frac{\kappa}{t} )$. To get an error of $\epsilon$, we must have $ t = \mathcal{O}( \frac{\kappa}{\epsilon} ) $. Since each iteration is $ \mathcal{O} (d)$, we have a computation time of $ \mathcal{O} ( \frac{\kappa d}{\epsilon} )$.

\end{itemize}

As a consequence:
\begin{itemize}
    \item When $n$ is large and $\epsilon$ not too small, GD will need more computation time to reach error $\epsilon$. An order of magnitude can be obtained by studying the value $\epsilon^*$ such that

	\begin{equation}
	   \kappa nd \log \frac{1}{\epsilon^*}  = \frac{\kappa d}{\epsilon^*} 
	\end{equation}

	Which translates to

	\begin{equation}
	    \epsilon^* \log \epsilon^* = -\frac{1}{n}  
	\end{equation}
    \item When $\epsilon \rightarrow 0$, GD becomes faster than SGD to reach this precision.
\end{itemize}

To conclude, for low precision and large $n$, SGD is a preferable. Also, in
machine learning, due to the estimation error that is $ \mathcal{O} (
\frac{1}{\sqrt{n} }) $, a very high precision is often not needed. Improvements
of the method exist, such as variance reduction methods, SAG or SAGA \cite[]{Schmidt2013}. 

\subsection{\large\color{MidnightBlue}Kernel methods}

\subsubsection{\large\color{Periwinkle}Context}

When using \textbf{{Kernel methods}}, we replace inputs $x\in  \mathcal{X} $ by a function $\phi(x)\in \mathcal{H}$, with $ \mathcal{H} $ a $ \mathbb{R} $-Hilbert space. We then perform linear predictions on $\phi(x)$. This means that estimators have the form: 

\begin{equation}
    f(x) = \langle\theta, \phi(x) \rangle_{ \mathcal{H} }
\end{equation}

\begin{itemize}
    \item $ \langle ., . \rangle_{ \mathcal{H} }$ is the inner product defined on $ \mathcal{H} $. When there is no ambiguity, we will note $ \langle ., . \rangle= \langle ., . \rangle_{ \mathcal{H} }$.
    \item $ \theta\in  \mathcal{H} $
    \item $ \phi(x)$ is called the \textbf{{feature}} associated to $x$, and $ \mathcal{H} $ is called the feature space.
\end{itemize}

Often, $ \mathcal{H} = \mathbb{R}^d $, but importantly, we will see that $d$ can even be \textbf{{infinite}}, thanks to a computation trick called the \textbf{{kernel trick}}. The reason why these methods are called "kernel methods" is not obvious at first glance but will be also justified after some calculations.  A \textit{{kernel}} will be a function of the form


$$
k  : \left\{
    \begin{array}{ll}
	\mathcal{H}^2  \rightarrow \mathbb{R}  \\
	(x,y) \mapsto \langle \phi(x), \phi(y) \rangle_{ \mathcal{H} } & 
    \end{array}
\right.
$$
\\


\textbf{{Examples of feature maps:}} 

\begin{itemize}
    \item \textbf{{Polynomial regression of degree d.}}

	$x\in \mathbb{R}$, $ \mathcal{H}  = \mathbb{R}^d$, 
	\begin{equation}
	    \phi(x) = (1, x, x^2, \dots, x^d)
	\end{equation}

	If $\theta\in \mathbb{R}^d$, 
	\begin{equation*}
	    f(x) = \theta^T\phi(x) = \sum^{r}_{j=1} \theta_jx^j
	\end{equation*}
    \item \textbf{{Polynomial multivariate regression of degree r.}}

	$x\in \mathbb{R}^p$, $ \mathcal{H}  = \mathbb{R}^d$.

	\begin{equation*}
	    \phi(x) = \{(x_1^{\alpha_1}\dots x_p^{\alpha_p}),{ \sum^{p}_{i=1} \alpha_i = r}\}
	\end{equation*}
	In that case, $d = \binom{d+r-1}{r}$.
    % \item \textbf{{Fourier Basis}}.

	% $\phi_1, \dots, \phi_d: \mathbb{R}^p\rightarrow \mathbb{R} $ with $\phi_k(x) = e^{i \frac{2\pi kx}{T}} $.

	% \begin{equation*}
	    % f(x) = \sum^{d}_{i=1} w_j\phi_j(x)
	% \end{equation*}
\end{itemize}

\subsubsection{\large\color{Periwinkle}Interests of kernel methods}

Kernel methods provide stable algorithms, with theoretical convergence guarantees. Importantly, they can benefit from the smoothness (regularity) of the target function, whereas local averaging methods cannot. They can be applied in high dimension.

Furthermore, using feature vectors allows us to process inputs that are not necessary vectors (for instance, texts).

In some supervised learning problems with many observations, such as computer vision and natural language processing, they are now outperformed by neural networks.

\subsubsection{\large\color{Periwinkle}Representer theorem}

We consider a framework where we look for a minimizer $ \hat{\theta}$ of a loss such as

\begin{equation}
    \label{eq:genloss}
    L( \theta) = \frac{1}{n} \sum^{n}_{i=1} l(y_i, \langle\theta,\phi(x_i)\rangle+\lambda ||\theta||^2
\end{equation}

With $\lambda>0$ and $\phi(x_i)\in \mathbb{R}^d$. The following theorem states that the minimizer belongs to the linear span of $\phi(x_i)$ (espace engendr√©), which is a subspace of $ \mathbb{R}^d$. This will have important consequences.

\begin{theorem}{Representer theorem}

    Let $\Psi : \mathbb{R}^{n+1}\rightarrow \mathbb{R} $ be a strictly increasing function with respect to the last variable. Then, the minimum of 

    \begin{equation*}
	L(\theta)  =\Psi(\langle\theta,\phi(x_i) \rangle, \dots, \langle\theta, \phi(x_n) \rangle, ||\theta||^2)
    \end{equation*}

    is attained for $ \hat{\theta}\in Vect(\{\phi(x_i)\}$. We can write
    
    \begin{equation*}
	\theta = \sum^{n}_{i=1} \alpha_i\phi(x_i), \alpha\in \mathbb{R}^n
    \end{equation*}
\end{theorem}

\begin{corollary}
    As a direct consequence, the minimum of loss \ref{eq:genloss} is attained at $\theta = \sum^{n}_{i=1} \alpha_i\phi(x_i), \alpha_i\in \mathbb{R}$.

    We note that no convexity hypothesis on $l$ is required.
\end{corollary}

\begin{proof}
    Let $ \mathcal{H}_D = \{  \sum^{n}_{i=1} \alpha_i\phi(x_i), \alpha\in \mathbb{R}^n\}$. For all $ \theta\in \mathcal{H} $, we have a decomposition $ \theta = \theta_D+ \theta_{D^{\perp}}$, with $ \theta_D \in \mathcal{H}_D $ and $ \theta_{D^{\perp}} \in \mathcal{H}_D^{\perp} $. ( \url{https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_du_suppl%C3%A9mentaire_orthogonal_d%27un_ferm%C3%A9_dans_un_espace_de_Hilbert} )

    We then know that $\forall i\in \{1, \dots, n\}$,
    
    \begin{equation*}
	\begin{aligned}
	    \langle \theta, \phi(x_i) \rangle &= \langle \theta_D, \phi(x_i) \rangle+ \langle \theta_{D^{\perp}}, \phi(x_i) \rangle\\
	    &= \langle \theta_D, \phi(x_i) \rangle
	\end{aligned}
    \end{equation*}
    Furthermore,
    \begin{equation*}
	|| \theta||^2 = || \theta_D||^2+ || \theta_{D^{\perp}}||^2
    \end{equation*}

    Hence

    \begin{equation*}
        \begin{aligned}
            \label{eq:}
	    \Psi(\langle\theta,\phi(x_i) \rangle, \dots, \langle\theta, \phi(x_n) \rangle, ||\theta||^2)&=\Psi(\langle\theta_D,\phi(x_i) \rangle, \dots, \langle\theta_D, \phi(x_n) \rangle, || \theta_D||^2+ || \theta_{D^{\perp}}||^2)\\
	    &\geq \Psi(\langle\theta_D,\phi(x_i) \rangle, \dots, \langle\theta_D, \phi(x_n) \rangle, || \theta_D||^2)\\
        \end{aligned}
    \end{equation*}

    This means that

    \begin{equation*}
	\inf_{ \theta\in \mathcal{H} }\Psi(\langle\theta,\phi(x_i) \rangle, \dots, \langle\theta, \phi(x_n) \rangle, ||\theta||^2)  = \inf_{ \theta\in \mathcal{H}_D }\Psi(\langle\theta,\phi(x_i) \rangle, \dots, \langle\theta, \phi(x_n) \rangle, ||\theta||^2)
    \end{equation*}
    
\end{proof}

\subsubsection{\large\color{Periwinkle}Consequence}

We note
\begin{itemize}
    \item $\alpha$ the vector such that $\theta = \sum^{n}_{i=1} \alpha_i\phi(x_i)$.
    \item $K\in \mathbb{R}^{n ,n}$ the matrix defined by
	\begin{equation}
	    K_{ij} = \langle\phi(x_i), \phi(x_j) \rangle 
	\end{equation}
\end{itemize}

Now, we remark that
\begin{equation*}
    \begin{aligned}
	||\theta||^2&= \langle\theta, \theta \rangle\\
	&= \langle \sum^{n}_{i=1} \alpha_i\phi(x_i) , \sum^{n}_{i=j} \alpha_j\phi(x_j)\rangle\\
	&= \sum^{n}_{i=1} \sum^{n}_{j=1} \alpha_i\alpha_j \langle\phi(x_i), \phi(x_j) \rangle\\
	&= \sum^{n}_{i=1} \alpha_i (\sum^{n}_{j=1} K_{ij}\alpha_j)\\
	&= \sum^{n}_{i=1} \alpha_i (K\alpha)_i\\
	&= \alpha^TK\alpha
    \end{aligned}
\end{equation*}

And that $\forall i\in [1, n]$,
\begin{equation*}
    \begin{aligned}
	\langle\theta, \phi(x_i) \rangle&= \sum^{n}_{j=1} \alpha_j \langle\phi(x_j),\phi(x_i) \rangle\\
	&= \sum^{n}_{j=1} \alpha_j K_{ij}\\
	&= (K\alpha)_{i}
    \end{aligned}
\end{equation*}

Finally

\begin{equation}
    L(\theta) = \frac{1}{n} \sum^{n}_{i=1} l(y_i, (K\alpha)_i)+\lambda \alpha^TK\alpha
\end{equation}

Hence, the loss can be written \textbf{{only with }} $K$ and $\alpha$, instead of the explicit $\phi(x_i)$. At first glance, this could seem like a deceitful statement, since we should need to know $\phi(x_i)$ and $\phi(x_j)$ in order to compute $k(x_i, x_j)$. \textbf{{However}}, in some situations, it is possible to compute $k(x_i, x_j)$ \textbf{{without explicit knowledge}} of $\phi$. This is known as the \textbf{{kernel trick}}.
\\

We can then define an alternate, equivalent minimization problem:

    \begin{equation}
	\begin{aligned}
	    \label{eq:}
	\inf_{ \theta\in \mathcal{H} } \frac{1}{n} \sum^{n}_{i=1} l(y_i, \langle\theta,\phi(x_i)\rangle+\lambda ||\theta||^2= \inf_{ \alpha\in \mathbb{R}^n} \frac{1}{n} \sum^{n}_{i=1} l(y_i, (K\alpha)_i)+\lambda \alpha^TK\alpha
	\end{aligned}
    \end{equation}

\textbf{{Consequences :}}
\begin{itemize}
    \item The initial optimization problem on $\theta$ can be replaced as an optimization problem on $\alpha$.
    \item If $d>>n$ (or even if $d$ is infinite, which can happen as we will see below), and if it is relatively fast to compute $k$, it might be easier to optimize on $\alpha \in \mathbb{R}^n$.  However, computing the matrix $K$ requires at least $ \mathcal{O} (n^2)$ operations.
    \item This allows to separate the representation problem (choice of the kernel) and the optimization problem (which uses the matrix $K$ as an input).
\end{itemize}

We can also write the evaluation function using $K$

\begin{equation}
    f(x) = \theta^T\phi(x) = \sum^{n}_{i=1} \alpha_ik(x_i,x)
\end{equation}


\subsubsection{\large\color{Periwinkle}Gram matrix}

The kernel matrix is a matrix of inner products. It is often called a Gram matrix. If we note the design matrix 

\begin{equation*}
\phi=
\begin{pmatrix}
    \phi(x_{1})^T\\
...\\
    \phi(x_{i})^T\\
...\\
    \phi(x_{n})^T
\end{pmatrix}
\end{equation*}

Then

\begin{equation}
    K = \phi \phi^T\in \mathbb{R}^{n, n}
\end{equation}

We can quickly show that it is a symmetric positive semi-definite matrix, as for all $\alpha\in \mathbb{R}^n$, 

\begin{equation}
    \begin{aligned}
        \label{eq:}
	\alpha K \alpha &= \alpha \phi \phi^T \alpha \\
	&= ( \phi^T \alpha)^T ( \phi^T \alpha)\\
	&= ||\phi^T \alpha||^2
    \end{aligned}
\end{equation}

Then, if $\lambda$ is an eigenvalue of $ K$, with eigenvector $\alpha_{\lambda}$, 

\begin{equation}
    \begin{aligned}
        \label{eq:}
	\alpha_{\lambda} K \alpha_{\lambda} &= \alpha_{\lambda} \lambda \alpha_{\lambda}\\
	&= \lambda ||\alpha_{\lambda}||^2
    \end{aligned}
\end{equation}

which shows that $\lambda\geq 0$.

Be careful not to mix $K = \phi \phi^T$ with the empirical covariance matrix \ref{eq:Sigma}

\begin{equation}
    \Sigma = \frac{1}{n}\phi^T \phi\in \mathbb{R}^{d, d}
\end{equation}

Note that in this definition, $d$ might still be infinite. With the same proof, $ \Sigma$ is positive semi-definite.

\subsubsection{\large\color{Periwinkle}Kernel functions}

More generally, we define \textbf{{kernel functions}}.

\begin{definition}{Kernel function}

    Let $ \mathcal{X} $ be a non-empty set. A function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is called a \textbf{{positive-definite kernel}} if and only if all kernel matrices are symmetric positive semi-definite.

\end{definition}

\begin{remark}

    No hypothesis is made on $ \mathcal{X} $.
    
\end{remark}

\begin{theorem}
    \label{th:aronszajn}
    A function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R} $ is a positive definite kernel if and only if there exists a $ \mathbb{R} $-Hilbert space $ \mathcal{H} $, with an inner product $ \langle .,. \rangle_{ \mathcal{H} }$, and a map $\phi: \mathcal{X} \rightarrow \mathcal{H} $ such that $\forall x, x'\in \mathcal{H} $,

    \begin{equation*}
	k(x,x') = \langle\phi(x),\phi(x') \rangle_{ \mathcal{H} }
    \end{equation*}
\end{theorem}

\begin{remark}
    Sometimes, the  property stated in theorem \ref{th:aronszajn} is used as a definition of a kernel function.
\end{remark}


\textbf{{Example:}} here is an example  where we can compute $k(x,x')$ without computing $\phi(x)$ explicitely. We consider $x\in ]-1, 1[$, and 
\begin{equation*}
    \phi(x) = (1, x, x^2, \dots, )
\end{equation*}

Then, 

\begin{equation*}
    \begin{aligned}
	k(x,x')&= \langle (1, x, x^2, \dots) , (1, x', x'^2, \dots) \rangle\\
	&=\sum_{i\in \mathbb{N}^*} x^ix'^i \\
	&=\sum_{i\in \mathbb{N}^*} (xx')^i \\
	&= \frac{1}{1-xx'} 
    \end{aligned}
\end{equation*}

Although $\phi(x)$ is \textbf{{infinite dimensional}}, we can compute $k(x,x')$ without evaluating and storing $\phi(x)$ in memory (which would be impossible anyways).

\subsubsection{\large\color{Periwinkle}Famous kernels}

\textbf{{Linear kernel}} 

\begin{equation}
    k(x,x') = x^Tx'
\end{equation}

% \subsubsection{\large\color{Periwinkle}Properties of kernels}
% 
% \begin{itemize}
%     \item sum
%     \item mapping
%     \item product
% \end{itemize}
% 
% \begin{itemize}
%     \item polynomial
%     \item L2
%     \item taylor series
%     \item exponential
%     \item gaussian
% \end{itemize}
% 
% \subsubsection{\large\color{Periwinkle}Mercer's theorem}

\section{\large\color{Blue}Unsupervised learning}

\subsection{\large\color{MidnightBlue}Definitions}

\subsubsection{\large\color{Periwinkle}Setup}

In an \textbf{{Unsupervised Leaning}} context, we are gien a dataset $\{x_i, i\in [1,n]\}$, from which we want to extract information. Most of the time the $x_i$ are vectors: $\forall i, x_i\in \mathbb{R}^d$. Most common applications include:
\begin{itemize}
    \item \textbf{{Clustering:}} identifying \textbf{{groups}} in the dataset by forming  \textbf{{partition}}.
    \item \textbf{{Dimensionality reduction}}: transforming the data $x_i$ into points $z_i$, most of time in a vector space of smaller dimension than the original space. The $z_i$ car then be used with less computational burden by a supervised learning algorithm, for instance.
    \item \textbf{{Density estimation}}: computing a useful approximation of the probabilty law that generated the dataset.
\end{itemize}


\subsection{\large\color{MidnightBlue}K-means clustering}

\subsubsection{\large\color{Periwinkle}Context}

$K$-means clustering is a \textbf{{vector quantization}} method. Vector  quantization associates \textbf{{prototypes}}, or \textbf{{centroids}}, to the data. Given a centroid $\mu_k\in \mathbb{R}^p$, the cluster $k$ is the set of all points for which the closest centroid is $\mu_k$.

The centroids $\mu_k$ are stored in a matrix $\mu$, the $z_i^k$ in matrix $z$.

$z_i^k$ is the indicator variable associated to $x_i$. If $x_i$ belongs to cluster $k$, $z_i^k=1$. Otherwise $z_i^k=0$.

\subsubsection{\large\color{Periwinkle}Distorsion}

\begin{definition}{Distorsion}
    \begin{equation*}
    J(\mu, z) = \sum^{n}_{i=1} \sum^{K}_{k=1} z_i^k||x_i-\mu_k||^2
    \end{equation*}
\end{definition}

The $K$-means minimizes the distorsion by \textbf{{Expectatoin-maximization algorithm}}.

\subsection{\large\color{MidnightBlue}Principal Component Analysis (PCA)}

\subsubsection{\large\color{Periwinkle}Context}

The PCA is a linear dimension reduction technique. Points in $ \mathbb{R}^d$ are linearly projected on a well chosen affine subspace.

\subsubsection{\large\color{Periwinkle}Optimal projection}

Without loss of generality, we assume the data are \textbf{{centered}}, which means that

\begin{equation}
\bar{x} = \sum^{n}_{i=1} x_n=0\in \mathbb{R}^d
\end{equation}

We note $X$ is the design matrix as in \ref{con:ols}. We look for $w\in \mathbb{R}^d$, with $||w||=1$, such that $ \hat{Var}(w^Tx)$ is maximal, where $ \hat{Var}$ denotes the empirical variance (see \ref{sec:empstats}).

\begin{proposition}
    $w$ is the eigenvector of $X^TX$ with largest eigenvalue $\lambda_{max}$.
\end{proposition}

\begin{proof}
    We want to maximize $ \hat{Var}(w^Tx)$. But as the data are centered, 

    \begin{equation}
            \label{eq:}
            \overline{w^Tx} = w^T \overline{x} = 0
    \end{equation}

    Hence,

    \begin{equation}
        \begin{aligned}
            \label{eq:}
	    \hat{Var}(w^Tx) &= \frac{1}{n-1}  \sum^{n}_{i=1} \big((w^Tx)_i- \overline{w^Tx}  \big)^2\\
	    &= \frac{1}{n-1}  \sum^{n}_{i=1} \big(w^Tx_i \big)^2\\
        \end{aligned}
    \end{equation}

    We thus want to maximize

    \begin{equation*}
        \begin{aligned}
	    \sum^{n}_{i=1} (w^Tx_i)^2 &= ||Xw||^2\\
	    &= \langle Xw, Xw \rangle\\
	    &= \langle (X^TX)w, w \rangle
        \end{aligned}
    \end{equation*}

    This quantity is always smaller that $\lambda_{max}$, and it attained for an eigenvector in the eigenspace with norm $1$, since we impose that $||w||=1$.

\end{proof}

Another point of view on the problem is to define $w$ as

\begin{equation*}
    w = \argmin_{||w||=1} \sum^{n}_{i=1} d(x_n, \text{Vect}(w))^2
\end{equation*}

\subsubsection{\large\color{Periwinkle}Algorithm}

Most of the time we project on a \textbf{{subspace}}, not only on a vector. The sequence of vectors obtained are the first normalized eigenvectors of $X^TX$, which are orthogonal.

\subsection{\large\color{MidnightBlue}Density etimation}
\label{sec:density}

\subsubsection{\large\color{Periwinkle}Context}

From points $ (x_1, \dots, x_n)\in \mathbb{R}^d$, we would like to estimate their unknown distribution probability $\rho$. What we have access to is the empirical distribution $ \rho_n$, 

\begin{equation*}
    \rho_n(x) = \frac{1}{n} \sum^{n}_{i=1} \delta_{x_i}
\end{equation*}

Where $\delta_{x_i}$ is the Dirac distribution in $x_i$.

\subsubsection{\large\color{Periwinkle}Kernel density estimation (KDE) and Convolution}

Let $q$ be a probability density on $ \mathbb{R}^d$, for instance, as $q(x) = e^{-||x||^2}$. Let $\tau>0$. We define a scaling of $q$ as 

\begin{equation*}
    q_{\tau}(x) = \tau^{-d} q( \frac{x}{\tau} )
\end{equation*}

We verify that $q_{\tau}$ is also a probabilty density.
\begin{equation*}
    \begin{aligned}
	\int_{ \mathbb{R}^d} q_{\tau}(x)& =\int_{ \mathbb{R}^d} \tau^{-d} q( \frac{x}{\tau} )\\
	&= 1
    \end{aligned}
\end{equation*}

We define the estimator $ \hat{\rho}_{\tau}$ as

\begin{equation*}
    \hat{\rho}_{\tau}(x) = \frac{1}{n} \sum^{n}_{i=1} q_{\tau}(x-x_i)
\end{equation*}

This estimation method is called \textbf{{Kernel density estimation (KDE)}}, or \textbf{{Parzen method}}. If $*$ is the convolution of $f$ and $g$, $(f*g)(x)= \int f(y)g(x-y)  dx $, then 

\begin{equation*}
    \hat{\rho}_{\tau}(x) =  (\rho_n * q_{\tau})(x)
\end{equation*}

% \subsubsection{\large\color{Periwinkle}Consistency of density estimation}
% 
% quantification biais et variance. On peut controler la qualit√© de l'estimation en fonction de n.
% 
% \begin{theorem}
%     \begin{equation*}
% 	E[|\rho(v)- \hat{\rho}(v)|^2]\leq \big( CT+ \sqrt{Q} \big)^2 n^{- \frac{2}{2+d} }
%     \end{equation*}
% \end{theorem}
% 
% We can show that with probability $1-\delta$,
% \begin{equation*}
%     \sup_{v\in[-R,R]^d} | \hat{\rho}(v)-\rho(v)| = \mathcal{O} \Big( n^{- \frac{1}{2+d} } \sqrt{\log( \frac{1}{\delta} + d \log( Rn)} \Big)
% \end{equation*}

\section{\large\color{Blue}Probabilistic modeling}

\subsection{\large\color{MidnightBlue}Context}

We are given a set of observations $\{y_1, \dots, y_n\}\in \mathcal{Y} $ that we assume are generated i.i.d from an unknown distribution. We look for a \textbf{{probabilistic model}} that explains well the data. We could use this model to predict well new data, that would be statistically similar to the observed ones.

\begin{definition}{Parametric model}

    Let $\mu$ be a measure on $ \mathcal{Y}$. Most of time times in Machine Learning, $\mu$ is the counting measure if $ \mathcal{R} \subset \mathbb{N} $ and the Lebesgue measure if $ \mathcal{Y} \subset \mathbb{R}^d$.

    Let $d>1$ and $ \Theta\subset \mathbb{R}^p$ be a set of parameters. A parametric model $ \mathcal{P} $ is a set of probability distributions on $ \mathcal{Y} $ with a density with respect to $\mu$, indexed by $ \Theta $.

    \begin{equation*}
	\mathcal{P}  = \{p_{\theta}d\mu|\theta \in \Theta \}
    \end{equation*}
\end{definition}

If we assume that the data were generated from some $p_{\theta^*}\in \mathcal{P} $, with a unknown parameter $ \theta^*$, our goal if to find a good estimation of $\theta$. If the data are indeed generated by a distribution in $ \mathcal{P} $, the problem is said to be \textbf{{well specified}}. Otherwise, the problem is said to be \textbf{{misspecified}}.

\subsection{\large\color{MidnightBlue}Maximum likelihood estimation}

\begin{definition}{Likelihood}

    Let $\mathcal{P}  = \{p_{\theta},\theta \in \Theta \}$ be a parametric model. Given $y\in \mathcal{Y} $, the \textbf{{likelihood}} of $\theta$ is defined as the function $ \theta\mapsto p_{\theta}(y)$.

    The likelihood $L(.|D_n)$ of a dataset $D_n = (y_1, \dots, y_n)$ is defined as

    \begin{equation*}
	L(.|D_n):\theta\mapsto \prod^{n}_{i=1} p_{\theta}(y_i)
    \end{equation*}

    The \textbf{{maximum likelihood estimator}}  (MLE) is the parameter $\theta$ that maximises the likelihood:

    \begin{equation*}
	\hat{\theta}_n \in \argmax_{\theta\in \Theta}( L(\theta|D_n)
    \end{equation*}

\end{definition}

\begin{remark}
    \begin{itemize}
        \item Since the samples $y_i$ are assumed to be independent, the likelihood corresponds to the probability of observing the dataset according to $p_{\theta}$.
	\item We often maximise the $\log$ of the likelihood, as it is easier to differentiate a sum. Since $\log$ is an increasing function, the MLE is also the maximiser of the $\log$ of $L$.
    \end{itemize}
\end{remark}

\subsubsection{\large\color{Periwinkle}Link with empirical risk minimization}

In the context of density estimation, we can define a loss function as the \textbf{{negative log-likelihood.}}
\begin{equation*}
    \Theta\times \mathcal{Y} \mapsto - \log(p_{\theta}(y))
\end{equation*}

Given this loss, the risk writes:
\begin{equation*}
    R(\theta) = E_Y[-\log\big(p_{\theta}(y)\big)]
\end{equation*}

and the empirical risk (ER):
\begin{equation*}
    R_n(\theta) =-\frac{1}{n} \sum^{n}_{i=1} \log\big(p_{\theta}(y_i)\big)]
\end{equation*}

The MLE is then also the empirical risk minimizer.

\subsubsection{\large\color{Periwinkle}Link with Kullback-Leibler divergence}

The Kullback-Leibler divergence is a quantity used to compare two probability distributions.

\begin{definition}{Kullback-Leibler divergence}

    Given two distributions $p$ and $q$, the KL divergence from $p$ to $q$ is defined as:

    \begin{equation*}
	KL(p||q) = E_{Y\sim p} \Big[\log \frac{p(Y)}{q(Y)} \Big]
    \end{equation*}
\end{definition}

\begin{lemma}
    
    If the data are generated by $p_{\theta^*}$, then $ KL(p_{\theta^*}||p_{\theta})$ is the excess risk of $p_{\theta}$, with the legative log-likelihood loss.
\end{lemma}

\subsubsection{\large\color{Periwinkle}Conditional modeling}

The likelihood can also be used to estimate the density of an output $Y$ given an output $X$. We then have a collection of densities on $Y$, for each $\theta$ and for each $x$. We also define a risk and an empirical risk the log-likelihood loss (also called \textbf{{conditional log-likelihood}} , as:

\begin{equation*}
    R(\theta) = E[-\log(p_{\theta}(Y|X)]
\end{equation*}

\begin{equation*}
    R_n(\theta) = - \frac{1}{n} \sum^{n}_{i=1} \log\big(p_{\theta}(y_i|x_i)\big)
\end{equation*}

\subsubsection{\large\color{Periwinkle}Link with linear regression}

We consider $D_n = \{(x_1, y_1), \dots, (x_n, y_n)\}\in (\mathcal{X}\times \mathcal{Y})^n$, with $ \mathcal{X} = \mathbb{R}^d$, and $ \mathcal{Y} = \mathbb{R} $. We can define a probabilistic model such that finding the maximum likelihood estimator for this model is equivalent to doing least square regression.

This model, called the \textbf{{Gaussian model}}, assumes that the outputs $y_i$ are independently generated from a Gaussian distribution of mean $w_*^Tx_i$ and variance $\sigma_*$. Out optimal parameter is thus $\theta^* =(w_*, \sigma_*)$. This writes:

\begin{equation*}
    Y = w_*^TX+\epsilon
\end{equation*}

where $\epsilon\sim \mathcal{N}(0, \sigma_*^2)$. We want an estimation of $\theta_*$. For a parameter $\theta=(w, \sigma)$, the conditional density is:

\begin{equation*}
    p_{\theta}(y|x) = \frac{1}{ \sqrt{2\pi} \sigma} e^{- \frac{(y-w^Tx)}{2\sigma^2} }
\end{equation*}

Hence 

\begin{equation*}
    \begin{aligned}
	R_n(\theta) &= - \frac{1}{n} \sum^{n}_{i=1} \log\big(p_{\theta}(y_i|x_i)\big)\\
	&= \frac{1}{2n\sigma^2} \sum^{n}_{i=1} (y_i-w^Tx_i)^2+ \frac{1}{2} \log(2\pi \sigma^2)
    \end{aligned}
\end{equation*}

This means that the maximum likelihood estimator $ \hat{w}_n$ in the Gaussian model is the minimiser of the least squares. We can also show that the estimator of $ \hat{\sigma}^2$ is

\begin{equation*}
    \hat{\sigma_n}^2 = \frac{1}{n} \sum^{n}_{i=1} (y_i- \hat{w}_n^Tx_i)^2
\end{equation*}
\\

\subsubsection{\large\color{Periwinkle}Link with logistic regression}

In a similar way, in a binary classification problem, we can introduce a model for which the maximum likelihood estimator is the logistic regression estimator $\hat{\theta}_{logit}$ defined in \ref{def:logicest}. This time, $ \mathcal{Y} = \{0, 1\}$, while we still have $ \mathcal{X}=\mathbb{R}^d$.

\begin{definition}{Sigmoid function}
$$
\sigma  : \left\{
    \begin{array}{ll}
	\mathbb{R} \rightarrow ]0, 1[ \\
	z \mapsto \frac{1}{1+e^{-z}} & 
    \end{array}
\right.
$$

We have $\forall z\in \mathbb{R} $,
\begin{equation*}
    \sigma(-z) = 1- \sigma(z)
\end{equation*}
and
\begin{equation*}
    \sigma'(z) = \sigma(z)\sigma(-z)
\end{equation*}
\end{definition}

Let us now consider the probabilistic model such that
\begin{equation*}
    p_{\theta}(1|x) = \sigma(\theta^Tx)
\end{equation*}

which makes sense since $\sigma(\theta^Tx)\in [0, 1]$, and can thus be interpreted as a probability. Equivalently, this model can be written (remember that $y=0$ or $y=1$)

\begin{equation*}
    p_{\theta}(y|x) = \big(\sigma(\theta^Tx)\big)^y\big(1-\sigma(\theta^Tx) \big)^{1-y}
\end{equation*}

Hence 

\begin{equation*}
    \begin{aligned}
	R_n(\theta) &= - \frac{1}{n} \sum^{n}_{i=1} \log\big(p_{\theta}(y_i|x_i)\big)\\
	&=- \frac{1}{n} \sum^{n}_{i=1} \log\Big(\big(\sigma(\theta^Tx_i)\big)^y_i\big(1-\sigma(\theta^Tx_i) \big)^{1-y_i}\Big)\\
	&=- \frac{1}{n} \sum^{n}_{i=1} y_i\log\Big(\sigma(\theta^Tx_i)\Big)+ (1-y_i)\log\Big(\sigma(-\theta^Tx_i)\Big)\\
	&=\frac{1}{n} \sum^{n}_{i=1} y_i\log\Big(1+e^{\theta^Tx_i}\Big)+ (1-y_i)\log\Big(1+e^{-\theta^Tx_i}\Big)\\
	&=\frac{1}{n} \sum^{n}_{i=1} l(\theta^Tx_i,y_i)
    \end{aligned}
\end{equation*}

where $l$ is the cross-entropy loss defined in \ref{loss:cross}. We find that the problems of empirical risk minimization for the log-likelihood in this model and the logistic regression with this formulation are the same.

\bibliographystyle{apalike}
\bibliography{../../../epita.bib} 

\end{document}
